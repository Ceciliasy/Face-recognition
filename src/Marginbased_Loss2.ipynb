{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarginBased_model.py\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import Block, HybridBlock\n",
    "import numpy as np\n",
    "\n",
    "class L2Normalization(HybridBlock):\n",
    "    r\"\"\"Applies L2 Normalization to input.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : str\n",
    "        Mode of normalization.\n",
    "        See :func:`~mxnet.ndarray.L2Normalization` for available choices.\n",
    "    Inputs:\n",
    "        - **data**: input tensor with arbitrary shape.\n",
    "    Outputs:\n",
    "        - **out**: output tensor with the same shape as `data`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mode, **kwargs):\n",
    "        self._mode = mode\n",
    "        super(L2Normalization, self).__init__(**kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.L2Normalization(x, mode=self._mode, name='l2_norm')\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}({_mode})'\n",
    "        return s.format(name=self.__class__.__name__,\n",
    "                        **self.__dict__)\n",
    "\n",
    "\n",
    "def get_distance(F, x):\n",
    "    \"\"\"Helper function for margin-based loss. Return a distance matrix given a matrix.\"\"\"\n",
    "    n = x.shape[0]\n",
    "\n",
    "    square = F.sum(x ** 2.0, axis=1, keepdims=True)\n",
    "    distance_square = square + square.transpose() - (2.0 * F.dot(x, x.transpose()))\n",
    "\n",
    "    # Adding identity to make sqrt work.\n",
    "    return F.sqrt(distance_square + F.array(np.identity(n)))\n",
    "\n",
    "class DistanceWeightedSampling(HybridBlock):\n",
    "    r\"\"\"Distance weighted sampling. See \"sampling matters in deep embedding learning\"\n",
    "    paper for details.\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_k : int\n",
    "        Number of images per class.\n",
    "    Inputs:\n",
    "        - **data**: input tensor with shape (batch_size, embed_dim).\n",
    "        Here we assume the consecutive batch_k examples are of the same class.\n",
    "        For example, if batch_k = 5, the first 5 examples belong to the same class,\n",
    "        6th-10th examples belong to another class, etc.\n",
    "    Outputs:\n",
    "        - a_indices: indices of anchors.\n",
    "        - x[a_indices]: sampled anchor embeddings.\n",
    "        - x[p_indices]: sampled positive embeddings.\n",
    "        - x[n_indices]: sampled negative embeddings.\n",
    "        - x: embeddings of the input batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_k, cutoff=0.5, nonzero_loss_cutoff=1.4, **kwargs):\n",
    "        self.batch_k = batch_k\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "        # We sample only from negatives that induce a non-zero loss.\n",
    "        # These are negatives with a distance < nonzero_loss_cutoff.\n",
    "        # With a margin-based loss, nonzero_loss_cutoff == margin + beta.\n",
    "        self.nonzero_loss_cutoff = nonzero_loss_cutoff\n",
    "        super(DistanceWeightedSampling, self).__init__(**kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        k = self.batch_k\n",
    "        n, d = x.shape\n",
    "\n",
    "        distance = get_distance(F, x)\n",
    "        # Cut off to avoid high variance.\n",
    "        distance = F.maximum(distance, self.cutoff)\n",
    "\n",
    "        # Subtract max(log(distance)) for stability.\n",
    "        log_weights = ((2.0 - float(d)) * F.log(distance)\n",
    "                       - (float(d - 3) / 2) * F.log(1.0 - 0.25 * (distance ** 2.0)))\n",
    "        weights = F.exp(log_weights - F.max(log_weights))\n",
    "\n",
    "        # Sample only negative examples by setting weights of\n",
    "        # the same-class examples to 0.\n",
    "        mask = np.ones(weights.shape)\n",
    "        for i in range(0, n, k):\n",
    "            mask[i:i+k, i:i+k] = 0\n",
    "\n",
    "        weights = weights * F.array(mask) * (distance < self.nonzero_loss_cutoff)\n",
    "        weights = weights / F.sum(weights, axis=1, keepdims=True)\n",
    "\n",
    "        a_indices = []\n",
    "        p_indices = []\n",
    "        n_indices = []\n",
    "\n",
    "        np_weights = weights.asnumpy()\n",
    "        for i in range(n):\n",
    "            block_idx = i // k\n",
    "\n",
    "            try:\n",
    "                n_indices += np.random.choice(n, k-1, p=np_weights[i]).tolist()\n",
    "            except:\n",
    "                n_indices += np.random.choice(n, k-1).tolist()\n",
    "            for j in range(block_idx * k, (block_idx + 1) * k):\n",
    "                if j != i:\n",
    "                    a_indices.append(i)\n",
    "                    p_indices.append(j)\n",
    "\n",
    "        return a_indices, x[a_indices], x[p_indices], x[n_indices], x\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}({batch_k})'\n",
    "        return s.format(name=self.__class__.__name__,\n",
    "                        **self.__dict__)\n",
    "\n",
    "\n",
    "class MarginNet(Block):\n",
    "    r\"\"\"Embedding network with distance weighted sampling.\n",
    "    It takes a base CNN and adds an embedding layer and a\n",
    "    sampling layer at the end.\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_net : Block\n",
    "        Base network.\n",
    "    emb_dim : int\n",
    "        Dimensionality of the embedding.\n",
    "    batch_k : int\n",
    "        Number of images per class in a batch. Used in sampling.\n",
    "    Inputs:\n",
    "        - **data**: input tensor with shape (batch_size, channels, width, height).\n",
    "        Here we assume the consecutive batch_k images are of the same class.\n",
    "        For example, if batch_k = 5, the first 5 images belong to the same class,\n",
    "        6th-10th images belong to another class, etc.\n",
    "    Outputs:\n",
    "        - The output of DistanceWeightedSampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_net, emb_dim, batch_k, **kwargs):\n",
    "        super(MarginNet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.base_net = base_net\n",
    "            self.dense = gluon.nn.Dense(emb_dim)\n",
    "            self.normalize = L2Normalization(mode='instance')\n",
    "            self.sampled = DistanceWeightedSampling(batch_k=batch_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.base_net(x)\n",
    "        z = self.dense(z)\n",
    "        z = self.normalize(z)\n",
    "        z = self.sampled(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class MarginLoss(gluon.loss.Loss):\n",
    "    r\"\"\"Margin based loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    margin : float\n",
    "        Margin between positive and negative pairs.\n",
    "    nu : float\n",
    "        Regularization parameter for beta.\n",
    "    Inputs:\n",
    "        - anchors: sampled anchor embeddings.\n",
    "        - positives: sampled positive embeddings.\n",
    "        - negatives: sampled negative embeddings.\n",
    "        - beta_in: class-specific betas.\n",
    "        - a_indices: indices of anchors. Used to get class-specific beta.\n",
    "    Outputs:\n",
    "        - Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.2, nu=0.0, weight=None, batch_axis=0, **kwargs):\n",
    "        super(MarginLoss, self).__init__(weight, batch_axis, **kwargs)\n",
    "        self._margin = margin\n",
    "        self._nu = nu\n",
    "\n",
    "    def hybrid_forward(self, F, anchors, positives, negatives, beta_in, a_indices=None):\n",
    "        if a_indices is not None:\n",
    "            # Jointly train class-specific beta.\n",
    "            beta = beta_in.data()[a_indices]\n",
    "            beta_reg_loss = F.sum(beta) * self._nu\n",
    "        else:\n",
    "            # Use a constant beta.\n",
    "            beta = beta_in\n",
    "            beta_reg_loss = 0.0\n",
    "\n",
    "        d_ap = F.sqrt(F.sum(F.square(positives - anchors), axis=1) + 1e-8)\n",
    "        d_an = F.sqrt(F.sum(F.square(negatives - anchors), axis=1) + 1e-8)\n",
    "\n",
    "        pos_loss = F.maximum(d_ap - beta + self._margin, 0.0)\n",
    "        neg_loss = F.maximum(beta - d_an + self._margin, 0.0)\n",
    "\n",
    "        pair_cnt = float(F.sum((pos_loss > 0.0) + (neg_loss > 0.0)).asscalar())\n",
    "\n",
    "        # Normalize based on the number of pairs.\n",
    "        loss = (F.sum(pos_loss + neg_loss) + beta_reg_loss) / pair_cnt\n",
    "        return gluon.loss._apply_weighting(F, loss, self._weight, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bottleneck\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ae/cedf5323f398ab4e4ff92d6c431a3e1c6a186f9b41ab3e8258dff786a290/Bottleneck-1.2.1.tar.gz (105kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from bottleneck) (1.13.3)\n",
      "Building wheels for collected packages: bottleneck\n",
      "  Running setup.py bdist_wheel for bottleneck ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/hb1500/.cache/pip/wheels/f2/bf/ec/e0f39aa27001525ad455139ee57ec7d0776fe074dfd78c97e4\n",
      "Successfully built bottleneck\n",
      "Installing collected packages: bottleneck\n",
      "Successfully installed bottleneck-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bottleneck import argpartition\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "from LFWDataset import LFWDataset\n",
    "import mxnet as mx\n",
    "#from data import cub200_iterator\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet import autograd as ag, nd\n",
    "#from model import MarginNet, MarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(alpha=0.5, batch_k=5, batch_size=70, beta=1.2, beta1=0.5, center_loss_weight=0.5, data_path='/scratch/hb1500/deeplearningdataset', data_test_path='/scratch/hb1500/deeplearningdataset/test', data_train_path='/scratch/hb1500/deeplearningdataset/train', dataroot='/scratch/hb1500/deeplearningdataset/train', embed_dim=128, embedding_size=512, epochs=20, factor=0.5, gpu_id='0', gpus='', kvstore='device', lfw_dir='/scratch/hb1500/lfw/lfw', lfw_pairs_path='lfw_pairs.txt', log_dir='/scratch/hb1500/logdir_triplet_loss', log_interval=20, lr=0.0001, lr_beta=0.1, lr_decay=0.0001, margin=0.2, model='resnet50_v2', n_triplets=1000000, no_cuda=False, nu=0.0, optimizer='adam', resume='/scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', save_model_prefix='margin_loss_model', seed=123, start_epoch=0, steps='12,14,16,18', test_batch_size=64, testdataroot='/scratch/hb1500/deeplearningdataset/test', use_pretrained=False, wd=0.0001)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# CLI\n",
    "parser = argparse.ArgumentParser(description='train a model for image classification.')\n",
    "parser.add_argument('--data-path', type=str, default='/scratch/hb1500/deeplearningdataset', #/scratch/hb1500/deeplearningdataset/train\n",
    "                    help='path of data.')\n",
    "\n",
    "###########\n",
    "parser.add_argument('--data_train-path', type=str, default='/scratch/hb1500/deeplearningdataset/train', \n",
    "                    help='path of data.')\n",
    "parser.add_argument('--data_test-path', type=str, default='/scratch/hb1500/deeplearningdataset/test', \n",
    "                    help='path of data.')\n",
    "###########\n",
    "\n",
    "parser.add_argument('--embed-dim', type=int, default=128,\n",
    "                    help='dimensionality of image embedding. default is 128.')\n",
    "parser.add_argument('--batch-size', type=int, default=70,\n",
    "                    help='training batch size per device (CPU/GPU). default is 70.')\n",
    "parser.add_argument('--batch-k', type=int, default=5,\n",
    "                    help='number of images per class in a batch. default is 5.')\n",
    "parser.add_argument('--gpus', type=str, default='',\n",
    "                    help='list of gpus to use, e.g. 0 or 0,2,5. empty means using cpu.')\n",
    "parser.add_argument('--epochs', type=int, default=20,\n",
    "                    help='number of training epochs. default is 20.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                    help='optimizer. default is adam.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='learning rate. default is 0.0001.')\n",
    "parser.add_argument('--lr-beta', type=float, default=0.1,\n",
    "                    help='learning rate for the beta in margin based loss. default is 0.1.')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                    help='margin for the margin based loss. default is 0.2.')\n",
    "parser.add_argument('--beta', type=float, default=1.2,\n",
    "                    help='initial value for beta. default is 1.2.')\n",
    "parser.add_argument('--nu', type=float, default=0.0,\n",
    "                    help='regularization parameter for beta. default is 0.0.')\n",
    "parser.add_argument('--factor', type=float, default=0.5,\n",
    "                    help='learning rate schedule factor. default is 0.5.')\n",
    "parser.add_argument('--steps', type=str, default='12,14,16,18',\n",
    "                    help='epochs to update learning rate. default is 12,14,16,18.')\n",
    "parser.add_argument('--wd', type=float, default=0.0001,\n",
    "                    help='weight decay rate. default is 0.0001.')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='random seed to use. default=123.')\n",
    "parser.add_argument('--model', type=str, default='resnet50_v2',\n",
    "                    help='type of model to use. see vision_model for options.')\n",
    "parser.add_argument('--save-model-prefix', type=str, default='margin_loss_model',\n",
    "                    help='prefix of models to be saved.')\n",
    "parser.add_argument('--use-pretrained', action='store_true',\n",
    "                    help='enable using pretrained model from gluon.')\n",
    "parser.add_argument('--kvstore', type=str, default='device',\n",
    "                    help='kvstore to use for trainer.')\n",
    "parser.add_argument('--log-interval', type=int, default=20,\n",
    "                    help='number of batches to wait before logging.')\n",
    "\n",
    "###################################\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/hb1500/deeplearningdataset/train',#default='/scratch/hb1500/deeplearningdataset/train'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/hb1500/deeplearningdataset/test',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/hb1500/lfw/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "\n",
    "parser.add_argument('--log-dir', default='/scratch/hb1500/logdir_triplet_loss',\n",
    "                    help='folder to output model checkpoints')\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "# parser.add_argument('--epochs', type=int, default=50, metavar='E',\n",
    "#                     help='number of epochs to train (default: 10)')\n",
    "# Training options\n",
    "# parser.add_argument('--embedding-size', type=int, default=256, metavar='ES',\n",
    "#                     help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--center_loss_weight', type=float, default=0.5, help='weight for center loss')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='learning rate of the centers')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "\n",
    "# parser.add_argument('--batch-size', type=int, default=64, metavar='BS',\n",
    "#                     help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--n-triplets', type=int, default=1000000, metavar='N',\n",
    "                    help='how many triplets will generate from the dataset,default=1000000')\n",
    "# parser.add_argument('--margin', type=float, default=1.0, metavar='MARGIN',\n",
    "#                     help='the margin value for the triplet loss function (default: 1.0')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "#                     help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "# parser.add_argument('--wd', default=0.0, type=float,\n",
    "#                     metavar='W', help='weight decay (default: 0.0)')\n",
    "# parser.add_argument('--optimizer', default='adam', type=str,\n",
    "#                     metavar='OPT', help='The optimizer to use (default: Adagrad)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "# parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "#                     help='random seed (default: 0)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "###################################\n",
    "opt = parser.parse_args(args = [])\n",
    "\n",
    "logging.info(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.\n",
    "mx.random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "\n",
    "batch_size = opt.batch_size\n",
    "\n",
    "gpus = [] if opt.gpus is None or opt.gpus is '' else [\n",
    "    int(gpu) for gpu in opt.gpus.split(',')]\n",
    "num_gpus = len(gpus)\n",
    "\n",
    "batch_size *= max(1, num_gpus)\n",
    "context = [mx.gpu(i) for i in gpus] if num_gpus > 0 else [mx.cpu()]\n",
    "steps = [int(step) for step in opt.steps.split(',')]\n",
    "\n",
    "# Construct model.\n",
    "kwargs = {'ctx': context, 'pretrained': opt.use_pretrained}\n",
    "net = models.get_model(opt.model, **kwargs)\n",
    "\n",
    "if opt.use_pretrained:\n",
    "    # Use a smaller learning rate for pre-trained convolutional layers.\n",
    "    for v in net.collect_params().values():\n",
    "        if 'conv' in v.name:\n",
    "            setattr(v, 'lr_mult', 0.01)\n",
    "\n",
    "net.hybridize()\n",
    "net = MarginNet(net.features, opt.embed_dim, opt.batch_k)\n",
    "beta = mx.gluon.Parameter('beta', shape=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:05<00:00, 1117.49it/s]\n"
     ]
    }
   ],
   "source": [
    "args = opt\n",
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "#LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "LOG_DIR = args.log_dir + '/run-optim_{}-n{}-lr{}-wd{}-m{}-embeddings{}-msceleb-alpha10'\\\n",
    "    .format(args.optimizer, args.n_triplets, args.lr, args.wd,\n",
    "            args.margin,args.embedding_size)\n",
    "\n",
    "# create logger\n",
    "#logger = Logger(LOG_DIR)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "#l2_dist = PairwiseDistance(2)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize(96),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "#train_dir = TripletFaceDataset(args.dataroot,transform=transform)\n",
    "#train_dir = TripletFaceDataset(dir=args.dataroot,n_triplets=args.n_triplets,transform=transform)\n",
    "#train_dir = MarginDataset(dir=args.dataroot,n_triplets=args.n_triplets,transform=transform)\n",
    "train_dir = ImageFolder(args.dataroot,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dir,batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get iterators.\n",
    "#train_data, val_data = cub200_iterator(opt.data_path, opt.batch_k, batch_size, (3, 224, 224))\n",
    "\n",
    "\n",
    "def get_distance_matrix(x):\n",
    "    \"\"\"Get distance matrix given a matrix. Used in testing.\"\"\"\n",
    "    square = nd.sum(x ** 2.0, axis=1, keepdims=True)\n",
    "    distance_square = square + square.transpose() - (2.0 * nd.dot(x, x.transpose()))\n",
    "    return nd.sqrt(distance_square)\n",
    "\n",
    "\n",
    "def evaluate_emb(emb, labels):\n",
    "    \"\"\"Evaluate embeddings based on Recall@k.\"\"\"\n",
    "    d_mat = get_distance_matrix(emb)\n",
    "    d_mat = d_mat.asnumpy()\n",
    "    labels = labels.asnumpy()\n",
    "\n",
    "    names = []\n",
    "    accs = []\n",
    "    for k in [1, 2, 4, 8, 16]:\n",
    "        names.append('Recall@%d' % k)\n",
    "        correct, cnt = 0.0, 0.0\n",
    "        for i in range(emb.shape[0]):\n",
    "            d_mat[i, i] = 1e10\n",
    "            nns = argpartition(d_mat[i], k)[:k]\n",
    "            if any(labels[i] == labels[nn] for nn in nns):\n",
    "                correct += 1\n",
    "            cnt += 1\n",
    "        accs.append(correct/cnt)\n",
    "    return names, accs\n",
    "\n",
    "\n",
    "def test(ctx):\n",
    "    \"\"\"Test a model.\"\"\"\n",
    "    val_data.reset()\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    for batch in val_data:\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        for x in data:\n",
    "            outputs.append(net(x)[-1])\n",
    "        labels += label\n",
    "\n",
    "    outputs = nd.concatenate(outputs, axis=0)[:val_data.n_test]\n",
    "    labels = nd.concatenate(labels, axis=0)[:val_data.n_test]\n",
    "    return evaluate_emb(outputs, labels)\n",
    "\n",
    "\n",
    "def get_lr(lr, epoch, steps, factor):\n",
    "    \"\"\"Get learning rate based on schedule.\"\"\"\n",
    "    for s in steps:\n",
    "        if epoch >= s:\n",
    "            lr *= factor\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(epochs, ctx):\n",
    "    \"\"\"Training function.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "\n",
    "    opt_options = {'learning_rate': opt.lr, 'wd': opt.wd}\n",
    "    if opt.optimizer == 'sgd':\n",
    "        opt_options['momentum'] = 0.9\n",
    "    if opt.optimizer == 'adam':\n",
    "        opt_options['epsilon'] = 1e-7\n",
    "    trainer = gluon.Trainer(net.collect_params(), opt.optimizer,\n",
    "                            opt_options,\n",
    "                            kvstore=opt.kvstore)\n",
    "    if opt.lr_beta > 0.0:\n",
    "        # Jointly train class-specific beta.\n",
    "        # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "        beta.initialize(mx.init.Constant(opt.beta), ctx=ctx)\n",
    "        trainer_beta = gluon.Trainer([beta], 'sgd',\n",
    "                                     {'learning_rate': opt.lr_beta, 'momentum': 0.9},\n",
    "                                     kvstore=opt.kvstore)\n",
    "\n",
    "    loss = MarginLoss(margin=opt.margin, nu=opt.nu)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        prev_loss, cumulative_loss = 0.0, 0.0\n",
    "\n",
    "        # Learning rate schedule.\n",
    "        trainer.set_learning_rate(get_lr(opt.lr, epoch, steps, opt.factor))\n",
    "        logging.info('Epoch %d learning rate=%f', epoch, trainer.learning_rate)\n",
    "        if opt.lr_beta > 0.0:\n",
    "            trainer_beta.set_learning_rate(get_lr(opt.lr_beta, epoch, steps, opt.factor))\n",
    "            logging.info('Epoch %d beta learning rate=%f', epoch, trainer_beta.learning_rate)\n",
    "            \n",
    "            \n",
    "        pbar = tqdm(enumerate(train_loader))\n",
    "        labels, distances = [], []\n",
    "            \n",
    "            \n",
    "        # Inner training loop.\n",
    "        #for i in range(200):\n",
    "        for batch_idx, (data,label) in pbar:\n",
    "#             batch = train_data.next()\n",
    "#             data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "#             label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#         data_v = Variable(data.cuda())\n",
    "#         target_var = Variable(label)\n",
    "            Ls = []\n",
    "            with ag.record():\n",
    "                for x, y in zip(data, label):\n",
    "                    a_indices, anchors, positives, negatives, _ = net(x)\n",
    "\n",
    "                    if opt.lr_beta > 0.0:\n",
    "                        L = loss(anchors, positives, negatives, beta, y[a_indices])\n",
    "                    else:\n",
    "                        L = loss(anchors, positives, negatives, opt.beta, None)\n",
    "\n",
    "                    # Store the loss and do backward after we have done forward\n",
    "                    # on all GPUs for better speed on multiple GPUs.\n",
    "                    Ls.append(L)\n",
    "                    cumulative_loss += nd.mean(L).asscalar()\n",
    "\n",
    "                for L in Ls:\n",
    "                    L.backward()\n",
    "\n",
    "            # Update.\n",
    "            trainer.step(batch.data[0].shape[0])\n",
    "            if opt.lr_beta > 0.0:\n",
    "                trainer_beta.step(batch.data[0].shape[0])\n",
    "\n",
    "            if (i+1) % opt.log_interval == 0:\n",
    "                logging.info('[Epoch %d, Iter %d] training loss=%f' % (\n",
    "                    epoch, i+1, cumulative_loss - prev_loss))\n",
    "                prev_loss = cumulative_loss\n",
    "\n",
    "        logging.info('[Epoch %d] training loss=%f'%(epoch, cumulative_loss))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "\n",
    "        names, val_accs = test(ctx)\n",
    "        for name, val_acc in zip(names, val_accs):\n",
    "            logging.info('[Epoch %d] validation: %s=%f'%(epoch, name, val_acc))\n",
    "\n",
    "        if val_accs[0] > best_val:\n",
    "            best_val = val_accs[0]\n",
    "            logging.info('Saving %s.' % opt.save_model_prefix)\n",
    "            net.collect_params().save('%s.params' % opt.save_model_prefix)\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_val_recall = train(opt.epochs, context)\n",
    "    print('Best validation Recall@1: %.2f.' % best_val_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
