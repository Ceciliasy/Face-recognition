{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarginBased_model.py\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import Block, HybridBlock\n",
    "import numpy as np\n",
    "\n",
    "class L2Normalization(HybridBlock):\n",
    "    r\"\"\"Applies L2 Normalization to input.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : str\n",
    "        Mode of normalization.\n",
    "        See :func:`~mxnet.ndarray.L2Normalization` for available choices.\n",
    "    Inputs:\n",
    "        - **data**: input tensor with arbitrary shape.\n",
    "    Outputs:\n",
    "        - **out**: output tensor with the same shape as `data`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mode, **kwargs):\n",
    "        self._mode = mode\n",
    "        super(L2Normalization, self).__init__(**kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.L2Normalization(x, mode=self._mode, name='l2_norm')\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}({_mode})'\n",
    "        return s.format(name=self.__class__.__name__,\n",
    "                        **self.__dict__)\n",
    "\n",
    "\n",
    "def get_distance(F, x):\n",
    "    \"\"\"Helper function for margin-based loss. Return a distance matrix given a matrix.\"\"\"\n",
    "    n = x.shape[0]\n",
    "\n",
    "    square = F.sum(x ** 2.0, axis=1, keepdims=True)\n",
    "    distance_square = square + square.transpose() - (2.0 * F.dot(x, x.transpose()))\n",
    "\n",
    "    # Adding identity to make sqrt work.\n",
    "    return F.sqrt(distance_square + F.array(np.identity(n)))\n",
    "\n",
    "class DistanceWeightedSampling(HybridBlock):\n",
    "    r\"\"\"Distance weighted sampling. See \"sampling matters in deep embedding learning\"\n",
    "    paper for details.\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_k : int\n",
    "        Number of images per class.\n",
    "    Inputs:\n",
    "        - **data**: input tensor with shape (batch_size, embed_dim).\n",
    "        Here we assume the consecutive batch_k examples are of the same class.\n",
    "        For example, if batch_k = 5, the first 5 examples belong to the same class,\n",
    "        6th-10th examples belong to another class, etc.\n",
    "    Outputs:\n",
    "        - a_indices: indices of anchors.\n",
    "        - x[a_indices]: sampled anchor embeddings.\n",
    "        - x[p_indices]: sampled positive embeddings.\n",
    "        - x[n_indices]: sampled negative embeddings.\n",
    "        - x: embeddings of the input batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_k, cutoff=0.5, nonzero_loss_cutoff=1.4, **kwargs):\n",
    "        self.batch_k = batch_k\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "        # We sample only from negatives that induce a non-zero loss.\n",
    "        # These are negatives with a distance < nonzero_loss_cutoff.\n",
    "        # With a margin-based loss, nonzero_loss_cutoff == margin + beta.\n",
    "        self.nonzero_loss_cutoff = nonzero_loss_cutoff\n",
    "        super(DistanceWeightedSampling, self).__init__(**kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        k = self.batch_k\n",
    "        n, d = x.shape\n",
    "\n",
    "        distance = get_distance(F, x)\n",
    "        # Cut off to avoid high variance.\n",
    "        distance = F.maximum(distance, self.cutoff)\n",
    "\n",
    "        # Subtract max(log(distance)) for stability.\n",
    "        log_weights = ((2.0 - float(d)) * F.log(distance)\n",
    "                       - (float(d - 3) / 2) * F.log(1.0 - 0.25 * (distance ** 2.0)))\n",
    "        weights = F.exp(log_weights - F.max(log_weights))\n",
    "\n",
    "        # Sample only negative examples by setting weights of\n",
    "        # the same-class examples to 0.\n",
    "        mask = np.ones(weights.shape)\n",
    "        for i in range(0, n, k):\n",
    "            mask[i:i+k, i:i+k] = 0\n",
    "\n",
    "        weights = weights * F.array(mask) * (distance < self.nonzero_loss_cutoff)\n",
    "        weights = weights / F.sum(weights, axis=1, keepdims=True)\n",
    "\n",
    "        a_indices = []\n",
    "        p_indices = []\n",
    "        n_indices = []\n",
    "\n",
    "        np_weights = weights.asnumpy()\n",
    "        for i in range(n):\n",
    "            block_idx = i // k\n",
    "\n",
    "            try:\n",
    "                n_indices += np.random.choice(n, k-1, p=np_weights[i]).tolist()\n",
    "            except:\n",
    "                n_indices += np.random.choice(n, k-1).tolist()\n",
    "            for j in range(block_idx * k, (block_idx + 1) * k):\n",
    "                if j != i:\n",
    "                    a_indices.append(i)\n",
    "                    p_indices.append(j)\n",
    "\n",
    "        return a_indices, x[a_indices], x[p_indices], x[n_indices], x\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}({batch_k})'\n",
    "        return s.format(name=self.__class__.__name__,\n",
    "                        **self.__dict__)\n",
    "\n",
    "\n",
    "class MarginNet(Block):\n",
    "    r\"\"\"Embedding network with distance weighted sampling.\n",
    "    It takes a base CNN and adds an embedding layer and a\n",
    "    sampling layer at the end.\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_net : Block\n",
    "        Base network.\n",
    "    emb_dim : int\n",
    "        Dimensionality of the embedding.\n",
    "    batch_k : int\n",
    "        Number of images per class in a batch. Used in sampling.\n",
    "    Inputs:\n",
    "        - **data**: input tensor with shape (batch_size, channels, width, height).\n",
    "        Here we assume the consecutive batch_k images are of the same class.\n",
    "        For example, if batch_k = 5, the first 5 images belong to the same class,\n",
    "        6th-10th images belong to another class, etc.\n",
    "    Outputs:\n",
    "        - The output of DistanceWeightedSampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_net, emb_dim, batch_k, **kwargs):\n",
    "        super(MarginNet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.base_net = base_net\n",
    "            self.dense = gluon.nn.Dense(emb_dim)\n",
    "            self.normalize = L2Normalization(mode='instance')\n",
    "            self.sampled = DistanceWeightedSampling(batch_k=batch_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.base_net(x)\n",
    "        z = self.dense(z)\n",
    "        z = self.normalize(z)\n",
    "        z = self.sampled(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class MarginLoss(gluon.loss.Loss):\n",
    "    r\"\"\"Margin based loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    margin : float\n",
    "        Margin between positive and negative pairs.\n",
    "    nu : float\n",
    "        Regularization parameter for beta.\n",
    "    Inputs:\n",
    "        - anchors: sampled anchor embeddings.\n",
    "        - positives: sampled positive embeddings.\n",
    "        - negatives: sampled negative embeddings.\n",
    "        - beta_in: class-specific betas.\n",
    "        - a_indices: indices of anchors. Used to get class-specific beta.\n",
    "    Outputs:\n",
    "        - Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.2, nu=0.0, weight=None, batch_axis=0, **kwargs):\n",
    "        super(MarginLoss, self).__init__(weight, batch_axis, **kwargs)\n",
    "        self._margin = margin\n",
    "        self._nu = nu\n",
    "\n",
    "    def hybrid_forward(self, F, anchors, positives, negatives, beta_in, a_indices=None):\n",
    "        if a_indices is not None:\n",
    "            # Jointly train class-specific beta.\n",
    "            beta = beta_in.data()[a_indices]\n",
    "            beta_reg_loss = F.sum(beta) * self._nu\n",
    "        else:\n",
    "            # Use a constant beta.\n",
    "            beta = beta_in\n",
    "            beta_reg_loss = 0.0\n",
    "\n",
    "        d_ap = F.sqrt(F.sum(F.square(positives - anchors), axis=1) + 1e-8)\n",
    "        d_an = F.sqrt(F.sum(F.square(negatives - anchors), axis=1) + 1e-8)\n",
    "\n",
    "        pos_loss = F.maximum(d_ap - beta + self._margin, 0.0)\n",
    "        neg_loss = F.maximum(beta - d_an + self._margin, 0.0)\n",
    "\n",
    "        pair_cnt = float(F.sum((pos_loss > 0.0) + (neg_loss > 0.0)).asscalar())\n",
    "\n",
    "        # Normalize based on the number of pairs.\n",
    "        loss = (F.sum(pos_loss + neg_loss) + beta_reg_loss) / pair_cnt\n",
    "        print('loss',loss)\n",
    "        return gluon.loss._apply_weighting(F, loss, self._weight, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/98/c9877e100c3d1ac92263bfaba7bb8a49294e099046592040a2ff8620ac61/mxnet-1.1.0.post0-py2.py3-none-manylinux1_x86_64.whl (23.8MB)\n",
      "\u001b[K    100% |################################| 23.8MB 869kB/s eta 0:00:01  7% |##                              | 1.9MB 17.6MB/s eta 0:00:02    22% |#######                         | 5.3MB 27.5MB/s eta 0:00:01    44% |##############                  | 10.6MB 14.0MB/s eta 0:00:01    51% |################                | 12.2MB 16.1MB/s eta 0:00:01    54% |#################               | 13.1MB 16.7MB/s eta 0:00:01    62% |####################            | 14.9MB 14.2MB/s eta 0:00:01    70% |######################          | 16.9MB 22.8MB/s eta 0:00:01    94% |##############################  | 22.4MB 25.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<2.19.0,>=2.18.4 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/requests-2.18.4-py3.6.egg (from mxnet) (2.18.4)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/44/21a7fdd50841aaaef224b943f7d10df87e476e181bb926ccf859bcb53d48/graphviz-0.8.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from mxnet) (1.13.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/chardet-3.0.4-py3.6.egg (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/idna-2.6-py3.6.egg (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/urllib3-1.22-py3.6.egg (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/certifi-2017.11.5-py3.6.egg (from requests<2.19.0,>=2.18.4->mxnet) (2017.11.5)\n",
      "\u001b[31mgrpcio 1.11.0 has requirement protobuf>=3.5.0.post1, but you'll have protobuf 3.4.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: graphviz, mxnet\n",
      "Successfully installed graphviz-0.8.3 mxnet-1.1.0.post0\n"
     ]
    }
   ],
   "source": [
    "#!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bottleneck import argpartition\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "from LFWDataset import LFWDataset\n",
    "import mxnet as mx\n",
    "#from data import cub200_iterator\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet import autograd as ag, nd\n",
    "#from model import MarginNet, MarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(alpha=0.5, batch_k=5, batch_size=70, beta=1.2, beta1=0.5, center_loss_weight=0.5, data_path='/scratch/ys3225/deeplearningdataset', data_test_path='/scratch/ys3225/deeplearningdataset/test', data_train_path='/scratch/ys3225/deeplearningdataset/train', dataroot='/scratch/ys3225/deeplearningdataset/train', embed_dim=128, embedding_size=512, epochs=20, factor=0.5, gpu_id='0', gpus='', kvstore='device', lfw_dir='/scratch/ys3225/lfw', lfw_pairs_path='lfw_pairs.txt', log_dir='/scratch/ys3225/logdir_triplet_loss', log_interval=20, lr=0.0001, lr_beta=0.1, lr_decay=0.0001, margin=0.2, model='resnet18_v1', n_triplets=1000000, no_cuda=False, nu=0.0, optimizer='adam', resume='/scratch/ys3225/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', save_model_prefix='margin_loss_model', seed=123, start_epoch=0, steps='12,14,16,18', test_batch_size=64, testdataroot='/scratch/ys3225/deeplearningdataset/test', use_pretrained=False, wd=0.0001)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# CLI\n",
    "parser = argparse.ArgumentParser(description='train a model for image classification.')\n",
    "parser.add_argument('--data-path', type=str, default='/scratch/ys3225/deeplearningdataset', #/scratch/hb1500/deeplearningdataset/train\n",
    "                    help='path of data.')\n",
    "\n",
    "###########\n",
    "parser.add_argument('--data_train-path', type=str, default='/scratch/ys3225/deeplearningdataset/train', \n",
    "                    help='path of data.')\n",
    "parser.add_argument('--data_test-path', type=str, default='/scratch/ys3225/deeplearningdataset/test', \n",
    "                    help='path of data.')\n",
    "###########\n",
    "\n",
    "parser.add_argument('--embed-dim', type=int, default=128,\n",
    "                    help='dimensionality of image embedding. default is 128.')\n",
    "parser.add_argument('--batch-size', type=int, default=70,\n",
    "                    help='training batch size per device (CPU/GPU). default is 70.')\n",
    "parser.add_argument('--batch-k', type=int, default=5,\n",
    "                    help='number of images per class in a batch. default is 5.')\n",
    "parser.add_argument('--gpus', type=str, default='',\n",
    "                    help='list of gpus to use, e.g. 0 or 0,2,5. empty means using cpu.')\n",
    "parser.add_argument('--epochs', type=int, default=20,\n",
    "                    help='number of training epochs. default is 20.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                    help='optimizer. default is adam.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='learning rate. default is 0.0001.')\n",
    "parser.add_argument('--lr-beta', type=float, default=0.1,\n",
    "                    help='learning rate for the beta in margin based loss. default is 0.1.')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                    help='margin for the margin based loss. default is 0.2.')\n",
    "parser.add_argument('--beta', type=float, default=1.2,\n",
    "                    help='initial value for beta. default is 1.2.')\n",
    "parser.add_argument('--nu', type=float, default=0.0,\n",
    "                    help='regularization parameter for beta. default is 0.0.')\n",
    "parser.add_argument('--factor', type=float, default=0.5,\n",
    "                    help='learning rate schedule factor. default is 0.5.')\n",
    "parser.add_argument('--steps', type=str, default='12,14,16,18',\n",
    "                    help='epochs to update learning rate. default is 12,14,16,18.')\n",
    "parser.add_argument('--wd', type=float, default=0.0001,\n",
    "                    help='weight decay rate. default is 0.0001.')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='random seed to use. default=123.')\n",
    "parser.add_argument('--model', type=str, default='resnet18_v1',\n",
    "                    help='type of model to use. see vision_model for options.resnet50_v2')\n",
    "parser.add_argument('--save-model-prefix', type=str, default='margin_loss_model',\n",
    "                    help='prefix of models to be saved.')\n",
    "parser.add_argument('--use-pretrained', action='store_true',\n",
    "                    help='enable using pretrained model from gluon.')\n",
    "parser.add_argument('--kvstore', type=str, default='device',\n",
    "                    help='kvstore to use for trainer.')\n",
    "parser.add_argument('--log-interval', type=int, default=20,\n",
    "                    help='number of batches to wait before logging.')\n",
    "\n",
    "###################################\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/ys3225/deeplearningdataset/train',#default='/scratch/hb1500/deeplearningdataset/train'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/ys3225/deeplearningdataset/test',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/ys3225/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "\n",
    "parser.add_argument('--log-dir', default='/scratch/ys3225/logdir_triplet_loss',\n",
    "                    help='folder to output model checkpoints')\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/ys3225/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "# parser.add_argument('--epochs', type=int, default=50, metavar='E',\n",
    "#                     help='number of epochs to train (default: 10)')\n",
    "# Training options\n",
    "# parser.add_argument('--embedding-size', type=int, default=256, metavar='ES',\n",
    "#                     help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--center_loss_weight', type=float, default=0.5, help='weight for center loss')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='learning rate of the centers')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "\n",
    "# parser.add_argument('--batch-size', type=int, default=64, metavar='BS',\n",
    "#                     help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--n-triplets', type=int, default=1000000, metavar='N',\n",
    "                    help='how many triplets will generate from the dataset,default=1000000')\n",
    "# parser.add_argument('--margin', type=float, default=1.0, metavar='MARGIN',\n",
    "#                     help='the margin value for the triplet loss function (default: 1.0')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "#                     help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "# parser.add_argument('--wd', default=0.0, type=float,\n",
    "#                     metavar='W', help='weight decay (default: 0.0)')\n",
    "# parser.add_argument('--optimizer', default='adam', type=str,\n",
    "#                     metavar='OPT', help='The optimizer to use (default: Adagrad)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "# parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "#                     help='random seed (default: 0)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "###################################\n",
    "opt = parser.parse_args(args = [])\n",
    "\n",
    "logging.info(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.\n",
    "mx.random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "\n",
    "batch_size = opt.batch_size\n",
    "\n",
    "gpus = [] if opt.gpus is None or opt.gpus is '' else [\n",
    "    int(gpu) for gpu in opt.gpus.split(',')]\n",
    "num_gpus = len(gpus)\n",
    "\n",
    "batch_size *= max(1, num_gpus)\n",
    "context = [mx.gpu(i) for i in gpus] if num_gpus > 0 else [mx.cpu()]\n",
    "steps = [int(step) for step in opt.steps.split(',')]\n",
    "\n",
    "# Construct model.\n",
    "kwargs = {'ctx': context, 'pretrained': opt.use_pretrained}\n",
    "net = models.get_model(opt.model, **kwargs)\n",
    "\n",
    "if opt.use_pretrained:\n",
    "    # Use a smaller learning rate for pre-trained convolutional layers.\n",
    "    for v in net.collect_params().values():\n",
    "        if 'conv' in v.name:\n",
    "            setattr(v, 'lr_mult', 0.01)\n",
    "\n",
    "net.hybridize()\n",
    "net = MarginNet(net.features, opt.embed_dim, opt.batch_k)\n",
    "beta = mx.gluon.Parameter('beta', shape=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:05<00:00, 1099.02it/s]\n"
     ]
    }
   ],
   "source": [
    "args = opt\n",
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "#LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "LOG_DIR = args.log_dir + '/run-optim_{}-n{}-lr{}-wd{}-m{}-embeddings{}-msceleb-alpha10'\\\n",
    "    .format(args.optimizer, args.n_triplets, args.lr, args.wd,\n",
    "            args.margin,args.embedding_size)\n",
    "\n",
    "# create logger\n",
    "#logger = Logger(LOG_DIR)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "#l2_dist = PairwiseDistance(2)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize(96),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "#train_dir = TripletFaceDataset(args.dataroot,transform=transform)\n",
    "#train_dir = TripletFaceDataset(dir=args.dataroot,n_triplets=args.n_triplets,transform=transform)\n",
    "#train_dir = MarginDataset(dir=args.dataroot,n_triplets=args.n_triplets,transform=transform)\n",
    "train_dir = ImageFolder(args.dataroot,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dir,batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get iterators.\n",
    "#train_data, val_data = cub200_iterator(opt.data_path, opt.batch_k, batch_size, (3, 224, 224))\n",
    "\n",
    "\n",
    "def get_distance_matrix(x):\n",
    "    \"\"\"Get distance matrix given a matrix. Used in testing.\"\"\"\n",
    "    square = nd.sum(x ** 2.0, axis=1, keepdims=True)\n",
    "    distance_square = square + square.transpose() - (2.0 * nd.dot(x, x.transpose()))\n",
    "    return nd.sqrt(distance_square)\n",
    "\n",
    "\n",
    "def evaluate_emb(emb, labels):\n",
    "    \"\"\"Evaluate embeddings based on Recall@k.\"\"\"\n",
    "    d_mat = get_distance_matrix(emb)\n",
    "    d_mat = d_mat.asnumpy()\n",
    "    labels = labels.asnumpy()\n",
    "\n",
    "    names = []\n",
    "    accs = []\n",
    "    for k in [1, 2, 4, 8, 16]:\n",
    "        names.append('Recall@%d' % k)\n",
    "        correct, cnt = 0.0, 0.0\n",
    "        for i in range(emb.shape[0]):\n",
    "            d_mat[i, i] = 1e10\n",
    "            nns = argpartition(d_mat[i], k)[:k]\n",
    "            if any(labels[i] == labels[nn] for nn in nns):\n",
    "                correct += 1\n",
    "            cnt += 1\n",
    "        accs.append(correct/cnt)\n",
    "    return names, accs\n",
    "\n",
    "\n",
    "def test(ctx):\n",
    "    \"\"\"Test a model.\"\"\"\n",
    "    val_data.reset()\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    for batch in val_data:\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        for x in data:\n",
    "            outputs.append(net(x)[-1])\n",
    "        labels += label\n",
    "\n",
    "    outputs = nd.concatenate(outputs, axis=0)[:val_data.n_test]\n",
    "    labels = nd.concatenate(labels, axis=0)[:val_data.n_test]\n",
    "    return evaluate_emb(outputs, labels)\n",
    "\n",
    "\n",
    "def get_lr(lr, epoch, steps, factor):\n",
    "    \"\"\"Get learning rate based on schedule.\"\"\"\n",
    "    for s in steps:\n",
    "        if epoch >= s:\n",
    "            lr *= factor\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(epochs, ctx):\n",
    "    \"\"\"Training function.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "\n",
    "    opt_options = {'learning_rate': opt.lr, 'wd': opt.wd}\n",
    "    if opt.optimizer == 'sgd':\n",
    "        opt_options['momentum'] = 0.9\n",
    "    if opt.optimizer == 'adam':\n",
    "        opt_options['epsilon'] = 1e-7\n",
    "    trainer = gluon.Trainer(net.collect_params(), opt.optimizer,\n",
    "                            opt_options,\n",
    "                            kvstore=opt.kvstore)\n",
    "    if opt.lr_beta > 0.0:\n",
    "        # Jointly train class-specific beta.\n",
    "        # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "        beta.initialize(mx.init.Constant(opt.beta), ctx=ctx)\n",
    "        trainer_beta = gluon.Trainer([beta], 'sgd',\n",
    "                                     {'learning_rate': opt.lr_beta, 'momentum': 0.9},\n",
    "                                     kvstore=opt.kvstore)\n",
    "\n",
    "    loss = MarginLoss(margin=opt.margin, nu=opt.nu)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        prev_loss, cumulative_loss = 0.0, 0.0\n",
    "\n",
    "        # Learning rate schedule.\n",
    "        trainer.set_learning_rate(get_lr(opt.lr, epoch, steps, opt.factor))\n",
    "        logging.info('Epoch %d learning rate=%f', epoch, trainer.learning_rate)\n",
    "        if opt.lr_beta > 0.0:\n",
    "            trainer_beta.set_learning_rate(get_lr(opt.lr_beta, epoch, steps, opt.factor))\n",
    "            logging.info('Epoch %d beta learning rate=%f', epoch, trainer_beta.learning_rate)\n",
    "            \n",
    "            \n",
    "        pbar = tqdm(enumerate(train_loader))\n",
    "        labels, distances = [], []\n",
    "            \n",
    "            \n",
    "        # Inner training loop.\n",
    "        #for i in range(200):\n",
    "        for batch_idx, (data,label) in pbar:\n",
    "#             batch = train_data.next()\n",
    "#             data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "#             label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#         data_v = Variable(data.cuda())\n",
    "#         target_var = Variable(label)\n",
    "            #Ls = []\n",
    "            with ag.record():\n",
    "                x = mx.ndarray.array(data.numpy())\n",
    "                y = mx.ndarray.array(label.numpy())\n",
    "                a_indices, anchors, positives, negatives, _ = net(x)\n",
    "\n",
    "                if opt.lr_beta > 0.0:\n",
    "                    L = loss(anchors, positives, negatives, beta, y[a_indices])\n",
    "                else:\n",
    "                    L = loss(anchors, positives, negatives, opt.beta, None)\n",
    "\n",
    "                # Store the loss and do backward after we have done forward\n",
    "                # on all GPUs for better speed on multiple GPUs.\n",
    "                #Ls.append(L)\n",
    "                cumulative_loss += nd.mean(L).asscalar()\n",
    "\n",
    "            #for L in Ls:\n",
    "                L.backward()\n",
    "\n",
    "            # Update.\n",
    "            trainer.step(x.shape[0])\n",
    "            if opt.lr_beta > 0.0:\n",
    "                trainer_beta.step(x.shape[0])\n",
    "\n",
    "            if (batch_idx) % opt.log_interval == 0:\n",
    "                #print(cumulative_loss)\n",
    "                logging.info('[Epoch %d, Iter %d] training loss=%f' % (\n",
    "                    epoch, batch_idx, cumulative_loss - prev_loss))\n",
    "                prev_loss = cumulative_loss\n",
    "\n",
    "        logging.info('[Epoch %d] training loss=%f'%(epoch, cumulative_loss))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "\n",
    "        names, val_accs = test(ctx)\n",
    "        for name, val_acc in zip(names, val_accs):\n",
    "            logging.info('[Epoch %d] validation: %s=%f'%(epoch, name, val_acc))\n",
    "\n",
    "        if val_accs[0] > best_val:\n",
    "            best_val = val_accs[0]\n",
    "            logging.info('Saving %s.' % opt.save_model_prefix)\n",
    "            net.collect_params().save('%s.params' % opt.save_model_prefix)\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_conv0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_batchnorm0_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_batchnorm0_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_batchnorm0_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_batchnorm0_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_conv0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm0_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm0_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm0_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm0_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_conv1_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm1_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm1_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm1_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm1_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_conv2_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm2_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm2_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm2_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm2_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_conv3_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm3_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm3_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm3_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage1_batchnorm3_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_conv0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm0_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm0_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm0_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm0_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_conv1_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm1_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm1_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm1_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm1_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_conv2_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm2_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm2_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm2_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm2_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_conv3_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm3_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm3_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm3_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm3_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_conv4_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm4_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm4_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm4_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage2_batchnorm4_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_conv0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm0_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm0_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm0_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm0_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_conv1_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm1_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm1_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm1_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm1_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_conv2_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm2_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm2_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm2_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm2_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_conv3_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm3_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm3_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm3_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm3_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_conv4_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm4_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm4_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm4_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage3_batchnorm4_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_conv0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm0_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm0_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm0_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm0_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_conv1_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm1_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm1_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm1_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm1_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_conv2_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm2_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm2_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm2_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm2_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_conv3_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm3_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm3_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm3_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm3_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_conv4_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm4_gamma is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm4_beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm4_running_mean is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter resnetv10_stage4_batchnorm4_running_var is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter marginnet0_dense0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter marginnet0_dense0_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/mxnet/gluon/parameter.py:320: UserWarning: Parameter beta is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "INFO:root:Epoch 0 learning rate=0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 0 beta learning rate=0.100000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[AINFO:root:[Epoch 0, Iter 0] training loss=0.885826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:03,  3.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2it [00:11,  5.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3it [00:19,  6.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4it [00:27,  6.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5it [00:34,  6.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6it [00:42,  7.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7it [00:51,  7.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8it [00:58,  7.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9it [01:06,  7.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "10it [01:14,  7.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "11it [01:22,  7.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "12it [01:29,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "13it [01:37,  7.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "14it [01:45,  7.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "15it [01:52,  7.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "16it [02:00,  7.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "17it [02:08,  7.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "18it [02:15,  7.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "19it [02:23,  7.56s/it]\u001b[A\u001b[A\u001b[A\u001b[AProcess Process-9:\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a269d75cc524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbest_val_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best validation Recall@1: %.2f.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_val_recall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-aa51b8f16999>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, ctx)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0ma_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3.6.3/lib/python3.6/site-packages/mxnet/ndarray/utils.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sparse_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3.6.3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source_array must be array like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2246\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3.6.3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_indexing_dispatch_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NDARRAY_BASIC_INDEXING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_nd_basic_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NDARRAY_ADVANCED_INDEXING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_nd_advanced_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3.6.3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_set_nd_basic_indexing\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    696\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_copyfrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# value might be a list or a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                     \u001b[0mvalue_nd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3.6.3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_sync_copyfrom\u001b[0;34m(self, source_array)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0msource_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             ctypes.c_size_t(source_array.size)))\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_val_recall = train(opt.epochs, context)\n",
    "    print('Best validation Recall@1: %.2f.' % best_val_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 0 learning rate=0.000100\n",
      "INFO:root:Epoch 0 beta learning rate=0.100000\n",
      "0it [00:00, ?it/s]INFO:root:[Epoch 0, Iter 0] training loss=0.930200\n",
      "1it [00:11, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.93019962]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.93019962]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 0.930199623108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:24, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.91740507]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.91740507]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:31, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.90673321]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.90673321]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:39,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.88220185]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.88220185]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:46,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.83398068]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.83398068]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:54,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.77277905]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.77277905]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [01:02,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.71019232]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.71019232]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [01:09,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.62252474]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.62252474]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [01:17,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.48916167]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.48916167]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [01:24,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.47051683]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.47051683]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [01:32,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.45091084]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.45091084]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [01:40,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.42601067]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.42601067]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [01:47,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.46469131]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.46469131]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [01:55,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.51586175]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.51586175]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [02:03,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 1.16150987]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 1.16150987]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [02:10,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 1.17238867]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 1.17238867]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [02:18,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 1.15615427]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 1.15615427]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [02:26,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 1.11623287]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 1.11623287]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [02:33,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 1.07199419]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 1.07199419]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [02:41,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.9871161]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.9871161]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0, Iter 20] training loss=16.120860\n",
      "21it [02:49,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.99249452]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.99249452]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 17.0510601103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [02:56,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.95787352]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.95787352]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [03:04,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.88952804]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.88952804]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [03:12,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.83331943]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.83331943]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [03:19,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.79198456]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.79198456]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [03:27,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.73027498]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.73027498]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [03:35,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.68877345]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.68877345]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [03:42,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.65354192]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.65354192]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [03:50,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.72846258]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.72846258]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [03:58,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.72060961]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.72060961]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [04:05,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.69318932]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.69318932]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [04:13,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.65480667]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.65480667]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [04:21,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.71406746]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.71406746]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [04:28,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.63331634]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.63331634]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [04:36,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.60383046]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.60383046]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [04:44,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.58724391]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.58724391]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [04:52,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.57116336]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.57116336]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [04:59,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.5626362]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.5626362]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [05:07,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.54934257]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.54934257]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [05:15,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.54553598]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.54553598]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0, Iter 40] training loss=13.661322\n",
      "41it [05:22,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.55182171]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.55182171]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 30.7123821676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [05:30,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.40666464]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.40666464]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [05:38,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.35743579]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.35743579]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [05:46,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.51043445]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.51043445]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [05:53,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.46853647]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.46853647]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [06:01,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.42070645]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.42070645]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [06:09,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.38056162]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.38056162]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [06:16,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.2944499]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.2944499]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [06:24,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.49778351]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.49778351]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [06:32,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.26283753]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.26283753]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "51it [06:39,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.31132311]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.31132311]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "52it [06:47,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.35376164]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.35376164]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "53it [06:55,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.46350756]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.46350756]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "54it [07:02,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.39023423]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.39023423]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55it [07:10,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.39793965]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.39793965]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "56it [07:18,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.32669663]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.32669663]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "57it [07:26,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.32369828]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.32369828]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "58it [07:33,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.3076607]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.3076607]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "59it [07:41,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.37766787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.37766787]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "60it [07:49,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.35369188]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.35369188]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0, Iter 60] training loss=7.451401\n",
      "61it [07:56,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.24580872]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.24580872]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 38.1637828052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "62it [08:04,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.25410056]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.25410056]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "63it [08:12,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.31967944]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.31967944]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "64it [08:19,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.26422262]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.26422262]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "65it [08:27,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20363738]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20363738]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "66it [08:35,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21358664]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21358664]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "67it [08:43,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.29319295]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.29319295]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "68it [08:50,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.40762755]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.40762755]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "69it [08:58,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.29034105]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.29034105]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "70it [09:06,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.28230765]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.28230765]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "71it [09:13,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.22582756]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.22582756]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "72it [09:21,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21078564]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21078564]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "73it [09:29,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19969577]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19969577]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "74it [09:37,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19925973]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19925973]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "75it [09:45,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19943261]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19943261]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "76it [09:52,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19932304]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19932304]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "77it [10:00,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19992433]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19992433]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "78it [10:08,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19931647]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19931647]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "79it [10:16,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19965608]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19965608]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "80it [10:23,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19983438]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19983438]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0, Iter 80] training loss=4.761119\n",
      "81it [10:31,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19936787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19936787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 42.9249021411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "82it [10:39,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19984844]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19984844]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "83it [10:46,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19936515]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19936515]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "84it [10:54,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.1985952]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.1985952]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "85it [11:02,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19936571]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19936571]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "86it [11:10,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19877286]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19877286]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "87it [11:17,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19966914]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19966914]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "88it [11:25,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19904248]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19904248]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "89it [11:33,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.1995123]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.1995123]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "90it [11:41,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19970463]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19970463]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "91it [11:48,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19948688]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19948688]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "92it [11:56,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19881736]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19881736]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "93it [12:04,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19972959]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19972959]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "94it [12:12,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19984749]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19984749]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "95it [12:20,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19863568]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19863568]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "96it [12:27,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19985077]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19985077]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "97it [12:35,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.22420447]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.22420447]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "98it [12:43,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19966483]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19966483]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "99it [12:50,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21588317]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21588317]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100it [12:58,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21884115]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21884115]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0, Iter 100] training loss=4.046547\n",
      "101it [13:06,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19770955]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19770955]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 46.9714489877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "102it [13:13,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21364801]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21364801]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "103it [13:21,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19925553]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19925553]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "104it [13:29,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20009357]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20009357]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "105it [13:36,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.2057824]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.2057824]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "106it [13:44,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.24411938]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.24411938]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "107it [13:52,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20065646]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20065646]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "108it [14:00,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.22489665]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.22489665]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "109it [14:07,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.26044676]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.26044676]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "110it [14:15,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20777147]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20777147]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "111it [14:23,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20863782]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20863782]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "112it [14:31,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21230014]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21230014]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "113it [14:38,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19798717]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19798717]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "114it [14:46,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21652737]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21652737]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "115it [14:54,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.2178912]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.2178912]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "116it [15:02,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.23770939]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.23770939]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "117it [15:09,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20031689]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20031689]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "118it [15:17,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20539396]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20539396]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "119it [15:25,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.23265149]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.23265149]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "120it [15:32,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.22567245]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.22567245]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0, Iter 120] training loss=4.307848\n",
      "121it [15:40,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19608991]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19608991]\n",
      "<NDArray 1 @cpu(0)>\n",
      "cumulative_loss 51.2792969793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "122it [15:48,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19886924]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19886924]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "123it [15:56,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.23855884]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.23855884]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "124it [16:03,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.25491846]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.25491846]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "125it [16:11,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.2227286]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.2227286]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "126it [16:19,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.1985635]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.1985635]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "127it [16:27,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19835052]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19835052]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "128it [16:35,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.25035492]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.25035492]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "129it [16:42,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.25866833]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.25866833]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "130it [16:50,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.21738748]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.21738748]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "131it [16:58,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19922917]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19922917]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "132it [17:05,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20233974]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20233974]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "133it [17:13,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.23033763]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.23033763]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "134it [17:21,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.24815291]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.24815291]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "135it [17:28,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.22015187]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.22015187]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "136it [17:36,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.24502853]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.24502853]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "137it [17:44,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.24900842]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.24900842]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "138it [17:52,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.24296364]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.24296364]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "139it [18:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.19890828]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.19890828]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "140it [18:07,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss \n",
      "[ 0.20485149]\n",
      "<NDArray 1 @cpu(0)>\n",
      "L \n",
      "[ 0.20485149]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "ctx =context\n",
    "epochs = 1\n",
    "\"\"\"Training function.\"\"\"\n",
    "if isinstance(ctx, mx.Context):\n",
    "    ctx = [ctx]\n",
    "net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "\n",
    "opt_options = {'learning_rate': opt.lr, 'wd': opt.wd}\n",
    "if opt.optimizer == 'sgd':\n",
    "    opt_options['momentum'] = 0.9\n",
    "if opt.optimizer == 'adam':\n",
    "    opt_options['epsilon'] = 1e-7\n",
    "trainer = gluon.Trainer(net.collect_params(), opt.optimizer,\n",
    "                        opt_options,\n",
    "                        kvstore=opt.kvstore)\n",
    "if opt.lr_beta > 0.0:\n",
    "    # Jointly train class-specific beta.\n",
    "    # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "    beta.initialize(mx.init.Constant(opt.beta), ctx=ctx)\n",
    "    trainer_beta = gluon.Trainer([beta], 'sgd',\n",
    "                                 {'learning_rate': opt.lr_beta, 'momentum': 0.9},\n",
    "                                 kvstore=opt.kvstore)\n",
    "\n",
    "loss = MarginLoss(margin=opt.margin, nu=opt.nu)\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    prev_loss, cumulative_loss = 0.0, 0.0\n",
    "\n",
    "    # Learning rate schedule.\n",
    "    trainer.set_learning_rate(get_lr(opt.lr, epoch, steps, opt.factor))\n",
    "    logging.info('Epoch %d learning rate=%f', epoch, trainer.learning_rate)\n",
    "    if opt.lr_beta > 0.0:\n",
    "        trainer_beta.set_learning_rate(get_lr(opt.lr_beta, epoch, steps, opt.factor))\n",
    "        logging.info('Epoch %d beta learning rate=%f', epoch, trainer_beta.learning_rate)\n",
    "\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    labels, distances = [], []\n",
    "\n",
    "\n",
    "    # Inner training loop.\n",
    "    #for i in range(200):\n",
    "    for batch_idx, (data,label) in pbar:\n",
    "#             batch = train_data.next()\n",
    "#             data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "#             label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#         data_v = Variable(data.cuda())\n",
    "#         target_var = Variable(label)\n",
    "        #Ls = []\n",
    "        with ag.record():\n",
    "            x = mx.ndarray.array(data.numpy())\n",
    "            y = mx.ndarray.array(label.numpy())\n",
    "            a_indices, anchors, positives, negatives, _ = net(x)\n",
    "\n",
    "            if opt.lr_beta > 0.0:\n",
    "                L = loss(anchors, positives, negatives, beta, y[a_indices])\n",
    "            else:\n",
    "                L = loss(anchors, positives, negatives, opt.beta, None)\n",
    "            #print('L',L)\n",
    "            # Store the loss and do backward after we have done forward\n",
    "            # on all GPUs for better speed on multiple GPUs.\n",
    "            #Ls.append(L)\n",
    "            cumulative_loss += nd.mean(L).asscalar()\n",
    "\n",
    "        #for L in Ls:\n",
    "            L.backward()\n",
    "\n",
    "        # Update.\n",
    "        trainer.step(x.shape[0])\n",
    "        if opt.lr_beta > 0.0:\n",
    "            trainer_beta.step(x.shape[0])\n",
    "\n",
    "        if (batch_idx) % opt.log_interval == 0:\n",
    "            #print('cumulative_loss',cumulative_loss)\n",
    "            logging.info('[Epoch %d, Iter %d] training loss=%f' % (\n",
    "                epoch, batch_idx, cumulative_loss - prev_loss))\n",
    "            prev_loss = cumulative_loss\n",
    "\n",
    "    logging.info('[Epoch %d] training loss=%f'%(epoch, cumulative_loss))\n",
    "    logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "\n",
    "    names, val_accs = test(ctx)\n",
    "    for name, val_acc in zip(names, val_accs):\n",
    "        logging.info('[Epoch %d] validation: %s=%f'%(epoch, name, val_acc))\n",
    "\n",
    "    if val_accs[0] > best_val:\n",
    "        best_val = val_accs[0]\n",
    "        logging.info('Saving %s.' % opt.save_model_prefix)\n",
    "        net.collect_params().save('%s.params' % opt.save_model_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
