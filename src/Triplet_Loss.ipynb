{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import FaceModel,FaceModelCenter,FaceModelSoftmax\n",
    "from eval_metrics import evaluate\n",
    "#from logger import Logger\n",
    "from LFWDataset import LFWDataset\n",
    "from TripletFaceDataset import TripletFaceDataset\n",
    "from PIL import Image\n",
    "from utils import PairwiseDistance,display_triplet_distance,display_triplet_distance_test\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Face Recognition')\n",
    "# Model options\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/hb1500/deeplearningdataset/train',#default='/scratch/hb1500/deeplearningdataset/train'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/hb1500/deeplearningdataset/test',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/hb1500/lfw/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "\n",
    "parser.add_argument('--log-dir', default='/scratch/hb1500/logdir_newdataset',\n",
    "                    help='folder to output model checkpoints')\n",
    "\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='E',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "# Training options\n",
    "# parser.add_argument('--embedding-size', type=int, default=256, metavar='ES',\n",
    "#                     help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--center_loss_weight', type=float, default=0.5, help='weight for center loss')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='learning rate of the centers')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='BS',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--n-triplets', type=int, default=1000000, metavar='N',\n",
    "                    help='how many triplets will generate from the dataset,default=1000000')\n",
    "parser.add_argument('--margin', type=float, default=1.0, metavar='MARGIN',\n",
    "                    help='the margin value for the triplet loss function (default: 1.0')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "parser.add_argument('--wd', default=0.0, type=float,\n",
    "                    metavar='W', help='weight decay (default: 0.0)')\n",
    "parser.add_argument('--optimizer', default='adam', type=str,\n",
    "                    metavar='OPT', help='The optimizer to use (default: Adagrad)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "                    help='random seed (default: 0)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4463/1000000 [00:00<00:22, 44622.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:23<00:00, 42483.04it/s]\n",
      "100%|██████████| 6000/6000 [00:03<00:00, 1770.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "#LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "LOG_DIR = args.log_dir + '/run-optim_{}-n{}-lr{}-wd{}-m{}-embeddings{}-msceleb-alpha10'\\\n",
    "    .format(args.optimizer, args.n_triplets, args.lr, args.wd,\n",
    "            args.margin,args.embedding_size)\n",
    "\n",
    "# create logger\n",
    "#logger = Logger(LOG_DIR)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "l2_dist = PairwiseDistance(2)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize(96),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "#train_dir = TripletFaceDataset(args.dataroot,transform=transform)\n",
    "train_dir = TripletFaceDataset(dir=args.dataroot,n_triplets=args.n_triplets,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dir,batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "#testacc_dir = TripletFaceDataset(dir=args.dataroot,n_triplets=args.n_triplets,transform=transform)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dir,batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "#testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "#    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed options:\n",
      "{'dataroot': '/scratch/hb1500/deeplearningdataset/train', 'testdataroot': '/scratch/hb1500/deeplearningdataset/test', 'lfw_dir': '/scratch/hb1500/lfw/lfw', 'lfw_pairs_path': 'lfw_pairs.txt', 'log_dir': '/scratch/hb1500/logdir_newdataset', 'resume': '/scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', 'start_epoch': 0, 'epochs': 50, 'center_loss_weight': 0.5, 'alpha': 0.5, 'embedding_size': 512, 'batch_size': 64, 'test_batch_size': 64, 'n_triplets': 1000000, 'margin': 1.0, 'lr': 0.001, 'beta1': 0.5, 'lr_decay': 0.0001, 'wd': 0.0, 'optimizer': 'adam', 'no_cuda': False, 'gpu_id': '0', 'seed': 0, 'log_interval': 10, 'cuda': True}\n",
      "\n",
      "\n",
      "Number of Classes:\n",
      "4988\n",
      "\n",
      "you are using gpu\n",
      "=> no checkpoint found at /scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [129280/1000000 (13%)]\tLoss: 9.080974 \t # of Selected Triplets: 27: : 2029it [1:26:14,  2.55s/it] "
     ]
    }
   ],
   "source": [
    "class TripletMarginLoss(Function):\n",
    "    \"\"\"Triplet loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin):\n",
    "        super(TripletMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pdist = PairwiseDistance(2)  # norm 2\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        d_p = self.pdist.forward(anchor, positive)\n",
    "        d_n = self.pdist.forward(anchor, negative)\n",
    "\n",
    "        dist_hinge = torch.clamp(self.margin + d_p - d_n, min=0.0)\n",
    "        loss = torch.mean(dist_hinge)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    labels, distances = [], []\n",
    "\n",
    "\n",
    "    for batch_idx, (data_a, data_p, data_n,label_p,label_n) in pbar:\n",
    "\n",
    "        data_a, data_p, data_n = data_a.cuda(), data_p.cuda(), data_n.cuda()\n",
    "        data_a, data_p, data_n = Variable(data_a), Variable(data_p), \\\n",
    "                                 Variable(data_n)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p, out_n = model(data_a), model(data_p), model(data_n)\n",
    "\n",
    "        # Choose the hard negatives\n",
    "        d_p = l2_dist.forward(out_a, out_p)\n",
    "        d_n = l2_dist.forward(out_a, out_n)\n",
    "#        all = (d_n - d_p < args.margin).cpu().data.numpy().flatten()\n",
    "        all_ = (d_n - d_p < args.margin).cpu().data.numpy().flatten()\n",
    "        hard_triplets = np.where(all_ == 1)\n",
    "        if len(hard_triplets[0]) == 0:\n",
    "            continue\n",
    "        out_selected_a = Variable(torch.from_numpy(out_a.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "        out_selected_p = Variable(torch.from_numpy(out_p.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "        out_selected_n = Variable(torch.from_numpy(out_n.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "\n",
    "        selected_data_a = Variable(torch.from_numpy(data_a.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "        selected_data_p = Variable(torch.from_numpy(data_p.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "        selected_data_n = Variable(torch.from_numpy(data_n.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "\n",
    "        selected_label_p = torch.from_numpy(label_p.cpu().numpy()[hard_triplets])\n",
    "        selected_label_n= torch.from_numpy(label_n.cpu().numpy()[hard_triplets])\n",
    "        triplet_loss = TripletMarginLoss(args.margin).forward(out_selected_a, out_selected_p, out_selected_n)\n",
    "\n",
    "        cls_a = model.forward_classifier(selected_data_a)\n",
    "        cls_p = model.forward_classifier(selected_data_p)\n",
    "        cls_n = model.forward_classifier(selected_data_n)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        predicted_labels = torch.cat([cls_a,cls_p,cls_n])\n",
    "        true_labels = torch.cat([Variable(selected_label_p.cuda()),Variable(selected_label_p.cuda()),Variable(selected_label_n.cuda())])\n",
    "\n",
    "        cross_entropy_loss = criterion(predicted_labels.cuda(),true_labels.cuda())\n",
    "\n",
    "        loss = cross_entropy_loss + triplet_loss\n",
    "        # compute gradient and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the optimizer learning rate\n",
    "        adjust_learning_rate(optimizer)\n",
    "\n",
    "        # log loss value\n",
    "        #logger.log_value('triplet_loss', triplet_loss.data[0]).step()\n",
    "        #logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "        #logger.log_value('total_loss', loss.data[0]).step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\t # of Selected Triplets: {}'.format(\n",
    "                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data[0],len(hard_triplets[0])))\n",
    "\n",
    "\n",
    "        dists = l2_dist.forward(out_selected_a,out_selected_n) #torch.sqrt(torch.sum((out_a - out_n) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(np.zeros(dists.size(0)))\n",
    "\n",
    "\n",
    "        dists = l2_dist.forward(out_selected_a,out_selected_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(np.ones(dists.size(0)))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "    print('\\33[91mTrain set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    #logger.log_value('Train Accuracy', np.mean(accuracy))\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.mkdir(LOG_DIR)\n",
    "    plot_roc(fpr,tpr,figure_name=\"roc_train_epoch_{}.png\".format(epoch))\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()},\n",
    "               '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "#     if not os.path.exists(LOG_DIR):\n",
    "#         os.mkdir(LOG_DIR)\n",
    "#     torch.save({'epoch': epoch + 1,\n",
    "#                 'state_dict': model.state_dict(),\n",
    "#                 'centers': model.centers},\n",
    "#             '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "        data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "                                Variable(data_p, volatile=True), Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p = model(data_a), model(data_p)\n",
    "        dists = l2_dist.forward(out_a,out_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(label.data.cpu().numpy())\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description('Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                100. * batch_idx / len(test_loader)))\n",
    "    #print(distances)\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "    print('\\33[91mTest set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    #logger.log_value('Test Accuracy', np.mean(accuracy))\n",
    "\n",
    "    plot_roc(fpr,tpr,figure_name=\"roc_test_epoch_{}.png\".format(epoch))\n",
    "\n",
    "def testaccuracy(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_value = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(top1.val[0]),float(top1.avg[0]))) \n",
    "    \n",
    "def plot_roc(fpr,tpr,figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(os.path.join(LOG_DIR,figure_name), dpi=fig.dpi)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0\n",
    "        group['step'] += 1\n",
    "\n",
    "        group['lr'] = args.lr / (1 + group['step'] * args.lr_decay)\n",
    "\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=args.wd, betas=(args.beta1, 0.999))\n",
    "    elif args.optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(),\n",
    "                                  lr=new_lr,\n",
    "                                  lr_decay=args.lr_decay,\n",
    "                                  weight_decay=args.wd)\n",
    "    return optimizer\n",
    "\n",
    "def main():\n",
    "    #test_display_triplet_distance= True\n",
    "    '''\n",
    "    why test_display_triplet_distance= True in center loss.py?????\n",
    "    '''\n",
    "    test_display_triplet_distance= True\n",
    "    # print the experiment configuration\n",
    "    print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "    print('\\nNumber of Classes:\\n{}\\n'.format(len(train_dir.classes)))\n",
    "\n",
    "    # instantiate model and initialize weights\n",
    "    #model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "    model = FaceModel(embedding_size=args.embedding_size,\n",
    "                      num_classes=len(train_dir.classes),\n",
    "                      pretrained=False)\n",
    "    if args.cuda:\n",
    "        print(\"you are using gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = create_optimizer(model, args.lr)\n",
    "    \n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print('=> loading checkpoint {}'.format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            checkpoint = None\n",
    "            print('=> no checkpoint found at {}'.format(args.resume))\n",
    "    print(checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    start = args.start_epoch\n",
    "    end = start + args.epochs\n",
    "\n",
    "    for epoch in range(start, end):\n",
    "        train(train_loader, model, optimizer, epoch)\n",
    "        test(test_loader, model, epoch)\n",
    "        testaccuracy(testaccuracy_loader, model, epoch)\n",
    "        if test_display_triplet_distance:\n",
    "            display_triplet_distance_test(model,test_loader,LOG_DIR+\"/test_{}\".format(epoch))\n",
    "            display_triplet_distance(model,train_loader,LOG_DIR+\"/train_{}\".format(epoch))\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
