{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import FaceModel,FaceModelCenter,FaceModelSoftmax,FaceModelMargin\n",
    "from eval_metrics import evaluate\n",
    "from logger import Logger\n",
    "from LFWDataset import LFWDataset\n",
    "from TrainDataset import TrainDataset\n",
    "from TripletFaceDataset import TripletFaceDataset\n",
    "from PIL import Image\n",
    "from utils import PairwiseDistance,display_triplet_distance,display_triplet_distance_test\n",
    "import collections\n",
    "\n",
    "## Marginloss\n",
    "from utils import PairwiseDistance\n",
    "from torch.autograd import Function\n",
    "class MarginLoss(Function):\n",
    "    r\"\"\"Margin based loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    margin : float\n",
    "        Margin between positive and negative pairs.\n",
    "    nu : float\n",
    "        Regularization parameter for beta.\n",
    "    Inputs:\n",
    "        - anchors: sampled anchor embeddings.\n",
    "        - positives: sampled positive embeddings.\n",
    "        - negatives: sampled negative embeddings.\n",
    "        - beta_in: class-specific betas.\n",
    "        - a_indices: indices of anchors. Used to get class-specific beta.\n",
    "    Outputs:\n",
    "        - Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.2, nu=0.0, weight=None, batch_axis=0, **kwargs):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.nu = nu\n",
    "        self.pdist = PairwiseDistance(2)\n",
    "        self.weight = weight\n",
    "    def forward(self,anchors, positives, negatives, beta_in, a_indices=None):\n",
    "        if a_indices is not None:\n",
    "            #确认beta_in是否需要是variable\n",
    "            # Jointly train class-specific beta.\n",
    "            beta = beta_in.index_select(0,a_indices)\n",
    "            beta_reg_loss = torch.sum(beta) * self.nu\n",
    "        else:\n",
    "            # Use a constant beta.\n",
    "            beta = beta_in\n",
    "            beta_reg_loss = 0.0\n",
    "            \n",
    "        d_p = self.pdist.forward(anchors, positives)\n",
    "        d_n = self.pdist.forward(anchors, negatives)\n",
    "#         d_ap = F.sqrt(F.sum(F.square(positives - anchors), axis=1) + 1e-8)\n",
    "#         d_an = F.sqrt(F.sum(F.square(negatives - anchors), axis=1) + 1e-8)\n",
    "        pos_loss = torch.clamp(self.margin + d_p - beta, min=0.0)\n",
    "        neg_loss = torch.clamp(self.margin - d_n + beta, min=0.0)\n",
    "        \n",
    "        pair_cnt = float(np.sum((pos_loss.cpu().data.numpy() > 0.0) + (neg_loss.cpu().data.numpy() > 0.0)))\n",
    "        # Normalize based on the number of pairs.\n",
    "        loss = (torch.sum(torch.pow(pos_loss,2) + torch.pow(neg_loss,2)) + beta_reg_loss) / pair_cnt\n",
    "        if self.weight:\n",
    "            loss = loss * self.weight\n",
    "        return loss\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# CLI\n",
    "parser = argparse.ArgumentParser(description='train a model for image classification.')\n",
    "###########\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/hb1500/Face_Aligned_6400/train_l/', \n",
    "                    help='path of data./scratch/ys3225/deeplearningdataset/train')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/hb1500/Face_Aligned_6670/test_l/', \n",
    "                    help='path of data.')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/ys3225/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "parser.add_argument('--log-dir', default='/scratch/ys3225/logdir_margin_loss',\n",
    "                    help='folder to output model checkpoints')\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/ys3225/logdir_margin_loss/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "parser.add_argument('--batch-size', type=int, default=64,\n",
    "                    help='training batch size per device (CPU/GPU). default is 70.')\n",
    "parser.add_argument('--batch-k', type=int, default=4,\n",
    "                    help='number of images per class in a batch. default is 5.')\n",
    "parser.add_argument('--gpus', type=str, default='4',\n",
    "                    help='list of gpus to use, e.g. 0 or 0,2,5. empty means using cpu.')\n",
    "parser.add_argument('--epochs', type=int, default=20,\n",
    "                    help='number of training epochs. default is 20.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                    help='optimizer. default is adam.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='learning rate. default is 0.0001.')\n",
    "parser.add_argument('--lr-beta', type=float, default=0.1,\n",
    "                    help='learning rate for the beta in margin based loss. default is 0.1.')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                    help='margin for the margin based loss. default is 0.2.')\n",
    "parser.add_argument('--beta', type=float, default=1.2,\n",
    "                    help='initial value for beta. default is 1.2.')\n",
    "parser.add_argument('--nu', type=float, default=0.0,\n",
    "                    help='regularization parameter for beta. default is 0.0.')\n",
    "parser.add_argument('--factor', type=float, default=0.5,\n",
    "                    help='learning rate schedule factor. default is 0.5.')\n",
    "parser.add_argument('--steps', type=str, default='12,14,16,18',\n",
    "                    help='epochs to update learning rate. default is 12,14,16,18.')\n",
    "parser.add_argument('--wd', type=float, default=0.0001,\n",
    "                    help='weight decay rate. default is 0.0001.')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='random seed to use. default=123.')\n",
    "parser.add_argument('--model', type=str, default='resnet18_v1',\n",
    "                    help='type of model to use. see vision_model for options.resnet50_v2')\n",
    "parser.add_argument('--save-model-prefix', type=str, default='margin_loss_model',\n",
    "                    help='prefix of models to be saved.')\n",
    "parser.add_argument('--use-pretrained', action='store_true',\n",
    "                    help='enable using pretrained model from gluon.')\n",
    "parser.add_argument('--kvstore', type=str, default='device',\n",
    "                    help='kvstore to use for trainer.')\n",
    "parser.add_argument('--log-interval', type=int, default=10,\n",
    "                    help='number of batches to wait before logging.')\n",
    "\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "# parser.add_argument('--wd', default=0.0, type=float,\n",
    "#                     metavar='W', help='weight decay (default: 0.0)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "logging.info(args)\n",
    "\n",
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "args.cuda = True\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "#LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "LOG_DIR = args.log_dir + '/run-margin-optim_{}-lr{}-m{}-embeddings{}-msceleb-alpha10'\\\n",
    "    .format(args.optimizer,args.lr,args.margin,args.embedding_size)\n",
    "    \n",
    "LOG_DIR_LOG = args.log_dir + '/logger'\n",
    "# create logger\n",
    "logger = Logger(LOG_DIR_LOG)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "l2_dist = PairwiseDistance(2)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize((96,96)),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "\n",
    "# train_dir = ImageFolder(args.dataroot,transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dir,batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "# testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "#                      transform=transform),\n",
    "#     batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "# testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dir,\n",
    "#    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainDataset(dir=args.dataroot,transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer,optim_beta,beta,epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    labels, distances = [], []\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for batch_idx, (data,label) in pbar:\n",
    "        data = Variable(data.cuda())\n",
    "        true_labels = Variable(label.cuda())\n",
    "        # compute output\n",
    "        # 需保证anchors, positives, negatives 是variable和cuda\n",
    "        x, a_indices, anchors, positives, negatives = model(data)\n",
    "        if args.lr_beta > 0.0:\n",
    "            margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, beta.cuda(), true_labels.index_select(0,a_indices))\n",
    "        else:\n",
    "            margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, args.beta, None)\n",
    "        predicted_labels = model.forward_classifier(data)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        true_labels = Variable(label.cuda())\n",
    "        \n",
    "        cross_entropy_loss = criterion(predicted_labels.cuda(),true_labels)\n",
    "\n",
    "        loss = cross_entropy_loss + margin_loss\n",
    "        # compute gradient and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if args.lr_beta > 0.0:\n",
    "             optim_beta.step()\n",
    "        # update the optimizer learning rate\n",
    "        adjust_learning_rate(optimizer)\n",
    "        # log loss value\n",
    "        logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "        logger.log_value('margin_loss', margin_loss.data[0]).step()\n",
    "        logger.log_value('total_loss', loss.data[0]).step()\n",
    "        prec = accuracy(predicted_labels.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data.size(0))\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data[0]))\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t'\n",
    "                'Train Prec@1 {:.2f} ({:.2f})\\n'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data[0],float(top1.val[0]), float(top1.avg[0])))\n",
    "        if batch_idx ==1:\n",
    "            break\n",
    "\n",
    "    # do checkpointing\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.mkdir(LOG_DIR)\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict()},\n",
    "            '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "        data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "                                Variable(data_p, volatile=True), Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p = model(data_a)[0], model(data_p)[0]\n",
    "        #print('out_a',out_a)\n",
    "        #print('out_p',out_p)\n",
    "        dists = l2_dist.forward(out_a,out_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        #print(dists)\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(label.data.cpu().numpy())\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description('Test LFW Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                100. * batch_idx / len(test_loader)))\n",
    "        if batch_idx == 1:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    #print(distances)\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "    print('\\33[91mTest LFW set: Verification Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    logger.log_value('Test LFW Accuracy', np.mean(accuracy))\n",
    "#     file = open('./log_triplet_loss/Verification_Accuracy.txt','a') \n",
    "#     file.write('\\33[91mTest set: Accuracy: {:.8f}\\n\\33[0m \\n'.format(np.mean(accuracy)))\n",
    "#     file.close()\n",
    "    plot_roc(fpr,tpr,figure_name=\"roc_test_epoch_{}.png\".format(epoch))\n",
    "\n",
    "def testaccuracy(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_value = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Recognition Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(top1.val[0]),float(top1.avg[0])))\n",
    "        if batch_idx == 1:\n",
    "            break\n",
    "    logger.log_value('Test batch Recognition Accuracy', float(top1.val[0]))          \n",
    "    logger.log_value('Test total Recognition Accuracy', float(top1.avg[0]))   \n",
    "    \n",
    "def testRecall(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data = Variable(data.cuda())\n",
    "        # compute output\n",
    "        out_data = model(data)[0]\n",
    "        distance_matrix = get_distance(out_data.cpu().data.numpy())\n",
    "        labels = label.cpu().numpy()\n",
    "        \n",
    "        names = []\n",
    "        accs = []\n",
    "        \n",
    "        for k in [1, 2, 4, 8, 16]:\n",
    "            names.append('Recall@%d' % k)\n",
    "            correct, cnt = 0.0, 0.0\n",
    "            for i in range(len(data)):\n",
    "                distance_matrix[i, i] = 1e10\n",
    "                nns = np.argpartition(distance_matrix[i], k)[:k]\n",
    "                if any(labels[i] == labels[nn] for nn in nns):\n",
    "                    correct += 1\n",
    "                cnt += 1\n",
    "            accs.append(correct/cnt)\n",
    "        \n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Recall@1\\t{:.2f} \\n'.format(\n",
    "                    epoch, batch_idx * len(data), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(accs[0])))\n",
    "#             file = open('./log_triplet_loss/Recognition_Recall.txt','a') \n",
    "#             file.write('Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "#                 'Test Recognition Prec@1 {:.2f} \\n'.format(\n",
    "#                     epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "#                     100. * batch_idx / len(test_loader),\n",
    "#                     float(accs[0])))\n",
    "#             file.close()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:                  \n",
    "            logger.log_value('Test Recall (l)', accs[0])\n",
    "            logger.log_value('Test Recall (2)', accs[1])\n",
    "            logger.log_value('Test Recall (4)', accs[2])\n",
    "            logger.log_value('Test Recall (8)', accs[3])\n",
    "            logger.log_value('Test Recall (16)', accs[4])            \n",
    "        if batch_idx == 1:\n",
    "            break\n",
    "def get_distance(x):\n",
    "    \"\"\"Helper function for margin-based loss. Return a distance matrix given a matrix.\"\"\"\n",
    "    n = x.shape[0]\n",
    "    square = np.sum(x ** 2,1,keepdims =True)\n",
    "    distance_square = square + square.T - (2.0 * np.dot(x, x.T))\n",
    "    # Adding identity to make sqrt work.\n",
    "    return np.sqrt(distance_square + np.identity(n))\n",
    "\n",
    "def plot_roc(fpr,tpr,figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(os.path.join(LOG_DIR,figure_name), dpi=fig.dpi)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0\n",
    "        group['step'] += 1\n",
    "\n",
    "        group['lr'] = args.lr / (1 + group['step'] * args.lr_decay)\n",
    "\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=args.wd, betas=(args.beta1, 0.999))\n",
    "    elif args.optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(),\n",
    "                                  lr=new_lr,\n",
    "                                  lr_decay=args.lr_decay,\n",
    "                                  weight_decay=args.wd)\n",
    "    return optimizer\n",
    "\n",
    "def main():\n",
    "    #test_display_triplet_distance= True\n",
    "    '''\n",
    "    why test_display_triplet_distance= True in center loss.py?????\n",
    "    '''\n",
    "    # test_display_triplet_distance= True\n",
    "    # print the experiment configuration\n",
    "    print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "    print('\\nNumber of Classes:\\n{}\\n'.format(str(6400)))\n",
    "    num_classes = 6400\n",
    "\n",
    "    # instantiate model and initialize weights\n",
    "    #model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "    model = FaceModelMargin(embedding_size=args.embedding_size,\n",
    "                      num_classes=num_classes,batch_k = args.batch_k, pretrained=False)\n",
    "    if args.cuda:\n",
    "        print(\"you are using gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = create_optimizer(model, args.lr)\n",
    "    if args.lr_beta > 0.0:\n",
    "        # Jointly train class-specific beta.\n",
    "        # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "        beta = nn.Parameter(nn.init.constant(torch.zeros(num_classes),args.beta))\n",
    "        optimizer_beta = optim.SGD([beta], lr=args.lr_beta,momentum=0.9)    \n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print('=> loading checkpoint {}'.format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            checkpoint = None\n",
    "            print('=> no checkpoint found at {}'.format(args.resume))\n",
    "    print(checkpoint)\n",
    "\n",
    "\n",
    "    start = args.start_epoch\n",
    "    end = start + args.epochs\n",
    "    end = 1\n",
    "    for epoch in range(start, end):\n",
    "        train(train_loader, model, optimizer,optimizer_beta,beta,epoch)\n",
    "        test(test_loader, model, epoch)\n",
    "        testaccuracy(testaccuracy_loader, model, epoch)\n",
    "        testRecall(testaccuracy_loader, model, epoch)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def main(): test of main\n",
    "#test_display_triplet_distance= True\n",
    "'''\n",
    "why test_display_triplet_distance= True in center loss.py?????\n",
    "'''\n",
    "test_display_triplet_distance= True\n",
    "# print the experiment configuration\n",
    "print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "print('\\nNumber of Classes:\\n{}\\n'.format(len(train_dir.classes)))\n",
    "\n",
    "# instantiate model and initialize weights\n",
    "#model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "model = FaceModelMargin(embedding_size=args.embedding_size,\n",
    "                  num_classes=len(train_dir.classes),batch_k = args.batch_k, pretrained=False)\n",
    "args.cuda = True\n",
    "if args.cuda:\n",
    "    print(\"you are using gpu\")\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = create_optimizer(model, args.lr)\n",
    "if args.lr_beta > 0.0:\n",
    "        # Jointly train class-specific beta.\n",
    "        # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "        beta = nn.Parameter(nn.init.constant(torch.zeros(len(train_dir.classes)),args.beta))\n",
    "        optimizer_beta = optim.SGD([beta], lr=args.lr_beta,momentum=0.9)\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "#     if args.resume:\n",
    "#         if os.path.isfile(args.resume):\n",
    "#             print('=> loading checkpoint {}'.format(args.resume))\n",
    "#             checkpoint = torch.load(args.resume)\n",
    "#             args.start_epoch = checkpoint['epoch']\n",
    "#         else:\n",
    "#             checkpoint = None\n",
    "#             print('=> no checkpoint found at {}'.format(args.resume))\n",
    "#     print(checkpoint)\n",
    "\n",
    "\n",
    "start = args.start_epoch\n",
    "end = start + args.epochs\n",
    "\n",
    "for epoch in range(start, end):\n",
    "    train(train_loader, model, optimizer,optimizer_beta, epoch)\n",
    "    test(test_loader, model, epoch)\n",
    "    testaccuracy(testaccuracy_loader, model, epoch)\n",
    "#     if test_display_triplet_distance:\n",
    "#         display_triplet_distance_test(model,test_loader,LOG_DIR+\"/test_{}\".format(epoch))\n",
    "#         display_triplet_distance(model,train_loader,LOG_DIR+\"/train_{}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test by cpu\n",
    "epoch = 1\n",
    "#def train(train_loader, model, optimizer, epoch):\n",
    "# switch to train mode\n",
    "model.train()\n",
    "\n",
    "pbar = tqdm(enumerate(train_loader))\n",
    "labels, distances = [], []\n",
    "\n",
    "\n",
    "for batch_idx, (data,label) in pbar:\n",
    "    data = Variable(data)#data = Variable(data.cuda())********************\n",
    "    # compute output\n",
    "    # 需保证anchors, positives, negatives 是variable和cuda\n",
    "    x, a_indices, anchors, positives, negatives = model(data)\n",
    "    # 暂时没有写beta的train， 后面再补\n",
    "    #args.lr_beta = -1\n",
    "    if args.lr_beta > 0.0:\n",
    "        margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, args.beta, label.index_select(0,a_indices.data))\n",
    "    else:\n",
    "        margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, args.beta, None)\n",
    "    # L should be a scalar\n",
    "    print(margin_loss)\n",
    "    #cumulative_loss += L\n",
    "        #for L in Ls:\n",
    "            #L.backward()\n",
    "# 加上训练Beta的optimizer，后一步再说\n",
    "#             # Update.\n",
    "#             trainer.step(x.shape[0])\n",
    "#             if opt.lr_beta > 0.0:\n",
    "#                 trainer_beta.step(x.shape[0])\n",
    "\n",
    "    predicted_labels = model.forward_classifier(data)#.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    true_labels = Variable(label)#.cuda())\n",
    "\n",
    "    cross_entropy_loss = criterion(predicted_labels,true_labels)\n",
    "\n",
    "    loss = cross_entropy_loss + margin_loss\n",
    "    # compute gradient and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update the optimizer learning rate\n",
    "    adjust_learning_rate(optimizer)\n",
    "\n",
    "    # log loss value\n",
    "    logger.log_value('triplet_loss', triplet_loss.data[0]).step()\n",
    "    logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "    logger.log_value('total_loss', loss.data[0]).step()\n",
    "    if batch_idx % args.log_interval == 0:\n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
