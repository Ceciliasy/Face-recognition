{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "class FaceModelMargin(nn.Module):\n",
    "    def __init__(self,embedding_size,num_classes,batch_k,pretrained=False):\n",
    "        super(FaceModelMargin, self).__init__()\n",
    "\n",
    "        self.model = resnet18(pretrained)\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.model.fc = nn.Linear(512*3*3, self.embedding_size)\n",
    "\n",
    "        self.model.classifier = nn.Linear(self.embedding_size, num_classes)\n",
    "        \n",
    "        self.batch_k = batch_k\n",
    "\n",
    "\n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.fc(x)\n",
    "        self.features = self.l2_norm(x)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        a_indices, x_a, x_p, x_n = DistanceWeightedSampling(self.features,self.batch_k)\n",
    "        alpha=10\n",
    "        self.features = self.features*alpha       \n",
    "\n",
    "        #x = self.model.classifier(self.features)\n",
    "        return self.features,a_indices, x_a, x_p, x_n\n",
    "\n",
    "    def forward_classifier(self, x):\n",
    "        features = (self.forward(x))[0]\n",
    "        res = self.model.classifier(features)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Weighted Sampling\n",
    "def get_distance(x):\n",
    "    \"\"\"Helper function for margin-based loss. Return a distance matrix given a matrix.\"\"\"\n",
    "    n = x.shape[0]\n",
    "    square = np.sum(x ** 2,1,keepdims =True)\n",
    "    distance_square = square + square.T - (2.0 * np.dot(x, x.T))\n",
    "    # Adding identity to make sqrt work.\n",
    "    return np.sqrt(distance_square + np.identity(n))\n",
    "\n",
    "def DistanceWeightedSampling(x,batch_k, cutoff=0.5, nonzero_loss_cutoff=1.4, **kwargs):\n",
    "    k = batch_k\n",
    "    n, d = x.data.shape\n",
    "    distance = get_distance(x.cpu().data.numpy())\n",
    "    # Cut off to avoid high variance.\n",
    "    distance = np.clip(distance,cutoff,None)\n",
    "    #distance[torch.lt(distance, cutoff)] = cutoff\n",
    "\n",
    "    # Subtract max(log(distance)) for stability.\n",
    "    log_weights = ((2.0 - float(d)) * np.log(distance)\n",
    "                   - (float(d - 3) / 2) * np.log(1.0 - 0.25 * (distance ** 2.0)))\n",
    "    weights = np.exp(log_weights - np.max(log_weights))\n",
    "\n",
    "    # Sample only negative examples by setting weights of\n",
    "    # the same-class examples to 0.\n",
    "    mask = np.ones(weights.shape)\n",
    "    for i in range(0, n, k):\n",
    "        mask[i:i+k, i:i+k] = 0\n",
    "\n",
    "    weights = weights * np.array(mask) * (distance < nonzero_loss_cutoff)\n",
    "    weights = weights / np.sum(weights, axis=1, keepdims=True)\n",
    "\n",
    "    a_indices = []\n",
    "    p_indices = []\n",
    "    n_indices = []\n",
    "\n",
    "    for i in range(n):\n",
    "        block_idx = i // k\n",
    "        try:\n",
    "            n_indices += np.random.choice(n, k-1, p=weights[i]).tolist()\n",
    "        except:\n",
    "            n_indices += np.random.choice(n, k-1).tolist()\n",
    "        for j in range(block_idx * k, (block_idx + 1) * k):\n",
    "            if j != i:\n",
    "                a_indices.append(i)\n",
    "                p_indices.append(j)\n",
    "#     GPU\n",
    "    a_indices = Variable(torch.LongTensor(a_indices).cuda())\n",
    "    p_indices = Variable(torch.LongTensor(p_indices).cuda())\n",
    "    n_indices = Variable(torch.LongTensor(n_indices).cuda())\n",
    "#     CPU\n",
    "#     a_indices = Variable(torch.LongTensor(a_indices))\n",
    "#     p_indices = Variable(torch.LongTensor(p_indices))\n",
    "#     n_indices = Variable(torch.LongTensor(n_indices))\n",
    "#     print(len(p_indices))\n",
    "#     print(len(n_indices))\n",
    "#     print(n,d)\n",
    "#     print(k)\n",
    "#     print(x.index_select(0,a_indices))\n",
    "#     print(x.index_select(0,p_indices))\n",
    "#     print(x.index_select(0,n_indices))\n",
    "    \n",
    "    return a_indices, x.index_select(0,a_indices), x.index_select(0,p_indices), x.index_select(0,n_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Marginloss\n",
    "from utils import PairwiseDistance\n",
    "from torch.autograd import Function\n",
    "class MarginLoss(Function):\n",
    "    r\"\"\"Margin based loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    margin : float\n",
    "        Margin between positive and negative pairs.\n",
    "    nu : float\n",
    "        Regularization parameter for beta.\n",
    "    Inputs:\n",
    "        - anchors: sampled anchor embeddings.\n",
    "        - positives: sampled positive embeddings.\n",
    "        - negatives: sampled negative embeddings.\n",
    "        - beta_in: class-specific betas.\n",
    "        - a_indices: indices of anchors. Used to get class-specific beta.\n",
    "    Outputs:\n",
    "        - Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.2, nu=0.0, weight=None, batch_axis=0, **kwargs):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.nu = nu\n",
    "        self.pdist = PairwiseDistance(2)\n",
    "        self.weight = weight\n",
    "    def forward(self,anchors, positives, negatives, beta_in, a_indices=None):\n",
    "        if a_indices is not None:\n",
    "            #确认beta_in是否需要是variable\n",
    "            # Jointly train class-specific beta.\n",
    "            beta = beta_in.index_select(0,a_indices)\n",
    "            beta_reg_loss = torch.sum(beta) * self.nu\n",
    "        else:\n",
    "            # Use a constant beta.\n",
    "            beta = beta_in\n",
    "            beta_reg_loss = 0.0\n",
    "            \n",
    "        d_p = self.pdist.forward(anchors, positives)\n",
    "        d_n = self.pdist.forward(anchors, negatives)\n",
    "#         d_ap = F.sqrt(F.sum(F.square(positives - anchors), axis=1) + 1e-8)\n",
    "#         d_an = F.sqrt(F.sum(F.square(negatives - anchors), axis=1) + 1e-8)\n",
    "        pos_loss = torch.clamp(self.margin + d_p - beta, min=0.0)\n",
    "        neg_loss = torch.clamp(self.margin - d_n + beta, min=0.0)\n",
    "        \n",
    "        pair_cnt = float(np.sum((pos_loss.cpu().data.numpy() > 0.0) + (neg_loss.cpu().data.numpy() > 0.0)))\n",
    "        # Normalize based on the number of pairs.\n",
    "        loss = (torch.sum(pos_loss + neg_loss) + beta_reg_loss) / pair_cnt\n",
    "        if self.weight:\n",
    "            loss = loss * self.weight\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import FaceModel,FaceModelCenter,FaceModelSoftmax\n",
    "from eval_metrics import evaluate\n",
    "#from logger import Logger\n",
    "from LFWDataset import LFWDataset\n",
    "from TripletFaceDataset import TripletFaceDataset\n",
    "from PIL import Image\n",
    "from utils import PairwiseDistance,display_triplet_distance,display_triplet_distance_test\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(alpha=0.5, batch_k=4, batch_size=64, beta=1.2, beta1=0.5, center_loss_weight=0.5, data_path='/scratch/ys3225/deeplearningdataset', data_test_path='/scratch/ys3225/deeplearningdataset/test', data_train_path='/scratch/ys3225/deeplearningdataset/train', dataroot='/scratch/ys3225/deeplearningdataset/train', embed_dim=128, embedding_size=512, epochs=20, factor=0.5, gpu_id='0', gpus='4', kvstore='device', lfw_dir='/scratch/ys3225/lfw', lfw_pairs_path='lfw_pairs.txt', log_dir='/scratch/ys3225/logdir_triplet_loss', log_interval=20, lr=0.0001, lr_beta=0.1, lr_decay=0.0001, margin=0.2, model='resnet18_v1', n_triplets=1000000, no_cuda=False, nu=0.0, optimizer='adam', resume='/scratch/ys3225/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', save_model_prefix='margin_loss_model', seed=123, start_epoch=0, steps='12,14,16,18', test_batch_size=64, testdataroot='/scratch/ys3225/deeplearningdataset/test', use_pretrained=False, wd=0.0001)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# CLI\n",
    "parser = argparse.ArgumentParser(description='train a model for image classification.')\n",
    "parser.add_argument('--data-path', type=str, default='/scratch/ys3225/deeplearningdataset', #/scratch/hb1500/deeplearningdataset/train\n",
    "                    help='path of data.')\n",
    "\n",
    "###########\n",
    "parser.add_argument('--data_train-path', type=str, default='/scratch/ys3225/deeplearningdataset/train', \n",
    "                    help='path of data.')\n",
    "parser.add_argument('--data_test-path', type=str, default='/scratch/ys3225/deeplearningdataset/test', \n",
    "                    help='path of data.')\n",
    "###########\n",
    "\n",
    "parser.add_argument('--embed-dim', type=int, default=128,\n",
    "                    help='dimensionality of image embedding. default is 128.')\n",
    "parser.add_argument('--batch-size', type=int, default=64,\n",
    "                    help='training batch size per device (CPU/GPU). default is 70.')\n",
    "parser.add_argument('--batch-k', type=int, default=4,\n",
    "                    help='number of images per class in a batch. default is 5.')\n",
    "parser.add_argument('--gpus', type=str, default='4',\n",
    "                    help='list of gpus to use, e.g. 0 or 0,2,5. empty means using cpu.')\n",
    "parser.add_argument('--epochs', type=int, default=20,\n",
    "                    help='number of training epochs. default is 20.')\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                    help='optimizer. default is adam.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='learning rate. default is 0.0001.')\n",
    "parser.add_argument('--lr-beta', type=float, default=0.1,\n",
    "                    help='learning rate for the beta in margin based loss. default is 0.1.')\n",
    "parser.add_argument('--margin', type=float, default=0.2,\n",
    "                    help='margin for the margin based loss. default is 0.2.')\n",
    "parser.add_argument('--beta', type=float, default=1.2,\n",
    "                    help='initial value for beta. default is 1.2.')\n",
    "parser.add_argument('--nu', type=float, default=0.0,\n",
    "                    help='regularization parameter for beta. default is 0.0.')\n",
    "parser.add_argument('--factor', type=float, default=0.5,\n",
    "                    help='learning rate schedule factor. default is 0.5.')\n",
    "parser.add_argument('--steps', type=str, default='12,14,16,18',\n",
    "                    help='epochs to update learning rate. default is 12,14,16,18.')\n",
    "parser.add_argument('--wd', type=float, default=0.0001,\n",
    "                    help='weight decay rate. default is 0.0001.')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='random seed to use. default=123.')\n",
    "parser.add_argument('--model', type=str, default='resnet18_v1',\n",
    "                    help='type of model to use. see vision_model for options.resnet50_v2')\n",
    "parser.add_argument('--save-model-prefix', type=str, default='margin_loss_model',\n",
    "                    help='prefix of models to be saved.')\n",
    "parser.add_argument('--use-pretrained', action='store_true',\n",
    "                    help='enable using pretrained model from gluon.')\n",
    "parser.add_argument('--kvstore', type=str, default='device',\n",
    "                    help='kvstore to use for trainer.')\n",
    "parser.add_argument('--log-interval', type=int, default=20,\n",
    "                    help='number of batches to wait before logging.')\n",
    "\n",
    "###################################\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/ys3225/deeplearningdataset/train',#default='/scratch/hb1500/deeplearningdataset/train'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/ys3225/deeplearningdataset/test',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/ys3225/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "\n",
    "parser.add_argument('--log-dir', default='/scratch/ys3225/logdir_triplet_loss',\n",
    "                    help='folder to output model checkpoints')\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/ys3225/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "# parser.add_argument('--epochs', type=int, default=50, metavar='E',\n",
    "#                     help='number of epochs to train (default: 10)')\n",
    "# Training options\n",
    "# parser.add_argument('--embedding-size', type=int, default=256, metavar='ES',\n",
    "#                     help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--center_loss_weight', type=float, default=0.5, help='weight for center loss')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='learning rate of the centers')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "\n",
    "# parser.add_argument('--batch-size', type=int, default=64, metavar='BS',\n",
    "#                     help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--n-triplets', type=int, default=1000000, metavar='N',\n",
    "                    help='how many triplets will generate from the dataset,default=1000000')\n",
    "# parser.add_argument('--margin', type=float, default=1.0, metavar='MARGIN',\n",
    "#                     help='the margin value for the triplet loss function (default: 1.0')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "#                     help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "# parser.add_argument('--wd', default=0.0, type=float,\n",
    "#                     metavar='W', help='weight decay (default: 0.0)')\n",
    "# parser.add_argument('--optimizer', default='adam', type=str,\n",
    "#                     metavar='OPT', help='The optimizer to use (default: Adagrad)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "# parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "#                     help='random seed (default: 0)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "###################################\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:05<00:00, 1069.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "args.cuda = True\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "#LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "LOG_DIR = args.log_dir + '/run-optim_{}-n{}-lr{}-wd{}-m{}-embeddings{}-msceleb-alpha10'\\\n",
    "    .format(args.optimizer, args.n_triplets, args.lr, args.wd,\n",
    "            args.margin,args.embedding_size)\n",
    "\n",
    "# create logger\n",
    "#logger = Logger(LOG_DIR)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "l2_dist = PairwiseDistance(2)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize(96),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "\n",
    "train_dir = ImageFolder(args.dataroot,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dir,batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed options:\n",
      "{'data_path': '/scratch/ys3225/deeplearningdataset', 'data_train_path': '/scratch/ys3225/deeplearningdataset/train', 'data_test_path': '/scratch/ys3225/deeplearningdataset/test', 'embed_dim': 128, 'batch_size': 64, 'batch_k': 4, 'gpus': '4', 'epochs': 20, 'optimizer': 'adam', 'lr': 0.0001, 'lr_beta': 0.1, 'margin': 0.2, 'beta': 1.2, 'nu': 0.0, 'factor': 0.5, 'steps': '12,14,16,18', 'wd': 0.0001, 'seed': 123, 'model': 'resnet18_v1', 'save_model_prefix': 'margin_loss_model', 'use_pretrained': False, 'kvstore': 'device', 'log_interval': 20, 'dataroot': '/scratch/ys3225/deeplearningdataset/train', 'testdataroot': '/scratch/ys3225/deeplearningdataset/test', 'lfw_dir': '/scratch/ys3225/lfw', 'lfw_pairs_path': 'lfw_pairs.txt', 'log_dir': '/scratch/ys3225/logdir_triplet_loss', 'resume': '/scratch/ys3225/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', 'start_epoch': 0, 'center_loss_weight': 0.5, 'alpha': 0.5, 'embedding_size': 512, 'test_batch_size': 64, 'n_triplets': 1000000, 'beta1': 0.5, 'lr_decay': 0.0001, 'no_cuda': False, 'gpu_id': '0', 'cuda': True}\n",
      "\n",
      "\n",
      "Number of Classes:\n",
      "4988\n",
      "\n",
      "you are using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer,optim_beta, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    labels, distances = [], []\n",
    "\n",
    "\n",
    "    for batch_idx, (data,label) in pbar:\n",
    "        data = Variable(data.cuda())\n",
    "        true_labels = Variable(label.cuda())\n",
    "        # compute output\n",
    "        # 需保证anchors, positives, negatives 是variable和cuda\n",
    "        print('*******')\n",
    "        x, a_indices, anchors, positives, negatives = model(data)\n",
    "        print(0)\n",
    "        if args.lr_beta > 0.0:\n",
    "            margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, beta.cuda(), true_labels.index_select(0,a_indices))\n",
    "        else:\n",
    "            margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, args.beta, None)\n",
    "\n",
    "# 加上训练Beta的optimizer，后一步再说\n",
    "#             # Update.\n",
    "#             trainer.step(x.shape[0])\n",
    "#             if opt.lr_beta > 0.0:\n",
    "#                 trainer_beta.step(x.shape[0])\n",
    "\n",
    "        predicted_labels = model.forward_classifier(data).cuda()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        true_labels = Variable(label.cuda())\n",
    "        \n",
    "        cross_entropy_loss = criterion(predicted_labels,true_labels)\n",
    "\n",
    "        loss = cross_entropy_loss + margin_loss\n",
    "        # compute gradient and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if args.lr_beta > 0.0:\n",
    "             optim_beta.step()\n",
    "        print(1)\n",
    "        # update the optimizer learning rate\n",
    "        adjust_learning_rate(optimizer)\n",
    "\n",
    "        # log loss value\n",
    "        #logger.log_value('triplet_loss', triplet_loss.data[0]).step()\n",
    "        #logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "        #logger.log_value('total_loss', loss.data[0]).step()\n",
    "        #if batch_idx % args.log_interval == 0:\n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0]))\n",
    "#             file = open('./log_triplet_loss/Train_Accuracy.txt','a') \n",
    "#             file.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t'\n",
    "#                 'Train Prec@1 {:.2f} ({:.2f})\\n'.format(\n",
    "#                     epoch, batch_idx * len(data_v), len(train_loader.dataset),\n",
    "#                     100. * batch_idx / len(train_loader),\n",
    "#                     loss.data[0],float(top1.val[0]), float(top1.avg[0])))\n",
    "#             file.close()\n",
    "        print(2)\n",
    "        if batch_idx ==1:\n",
    "            break\n",
    "\n",
    "# 评价train 的AUC， 后一步再说\n",
    "#         dists = l2_dist.forward(out_selected_a,out_selected_n) #torch.sqrt(torch.sum((out_a - out_n) ** 2, 1))  # euclidean distance\n",
    "#         distances.append(dists.data.cpu().numpy())\n",
    "#         labels.append(np.zeros(dists.size(0)))\n",
    "\n",
    "\n",
    "#         dists = l2_dist.forward(out_selected_a,out_selected_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "#         distances.append(dists.data.cpu().numpy())\n",
    "#         labels.append(np.ones(dists.size(0)))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#     labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "#     distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "#     tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "#     print('\\33[91mTrain set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "#     #logger.log_value('Train Accuracy', np.mean(accuracy))\n",
    "#     if not os.path.exists(LOG_DIR):\n",
    "#         os.mkdir(LOG_DIR)\n",
    "#     plot_roc(fpr,tpr,figure_name=\"roc_train_epoch_{}.png\".format(epoch))\n",
    "\n",
    "#     # do checkpointing\n",
    "#     torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()},\n",
    "#                '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "#     if not os.path.exists(LOG_DIR):\n",
    "#         os.mkdir(LOG_DIR)\n",
    "#     torch.save({'epoch': epoch + 1,\n",
    "#                 'state_dict': model.state_dict(),\n",
    "#                 'centers': model.centers},\n",
    "#             '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "        data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "                                Variable(data_p, volatile=True), Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p = model(data_a)[0], model(data_p)[0]\n",
    "        #print('out_a',out_a)\n",
    "        #print('out_p',out_p)\n",
    "        dists = l2_dist.forward(out_a,out_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        #print(dists)\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(label.data.cpu().numpy())\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description('Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                100. * batch_idx / len(test_loader)))\n",
    "        if batch_idx == 1:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    #print(distances)\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "    print('\\33[91mTest Verification set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "#     #logger.log_value('Test Accuracy', np.mean(accuracy))\n",
    "#     file = open('./log_triplet_loss/Verification_Accuracy.txt','a') \n",
    "#     file.write('\\33[91mTest set: Accuracy: {:.8f}\\n\\33[0m \\n'.format(np.mean(accuracy)))\n",
    "#     file.close()\n",
    "#     plot_roc(fpr,tpr,figure_name=\"roc_test_epoch_{}.png\".format(epoch))\n",
    "\n",
    "def testaccuracy(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_value = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Recognition Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(top1.val[0]),float(top1.avg[0])))\n",
    "#             file = open('./log_triplet_loss/Recognition_Accuracy.txt','a') \n",
    "#             file.write('Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "#                 'Test Recognition Prec@1 {:.2f} ({:.2f}) \\n'.format(\n",
    "#                     epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "#                     100. * batch_idx / len(test_loader),\n",
    "#                     float(top1.val[0]),float(top1.avg[0])))\n",
    "#             file.close()\n",
    "        if batch_idx == 1:\n",
    "            break\n",
    "            \n",
    "    \n",
    "def plot_roc(fpr,tpr,figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(os.path.join(LOG_DIR,figure_name), dpi=fig.dpi)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0\n",
    "        group['step'] += 1\n",
    "\n",
    "        group['lr'] = args.lr / (1 + group['step'] * args.lr_decay)\n",
    "\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=args.wd, betas=(args.beta1, 0.999))\n",
    "    elif args.optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(),\n",
    "                                  lr=new_lr,\n",
    "                                  lr_decay=args.lr_decay,\n",
    "                                  weight_decay=args.wd)\n",
    "    return optimizer\n",
    "\n",
    "def main():\n",
    "    #test_display_triplet_distance= True\n",
    "    '''\n",
    "    why test_display_triplet_distance= True in center loss.py?????\n",
    "    '''\n",
    "    # test_display_triplet_distance= True\n",
    "    # print the experiment configuration\n",
    "    print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "    print('\\nNumber of Classes:\\n{}\\n'.format(len(train_dir.classes)))\n",
    "\n",
    "    # instantiate model and initialize weights\n",
    "    #model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "    model = FaceModelMargin(embedding_size=args.embedding_size,\n",
    "                      num_classes=len(train_dir.classes),batch_k = args.batch_k, pretrained=False)\n",
    "    if args.cuda:\n",
    "        print(\"you are using gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = create_optimizer(model, args.lr)\n",
    "    if args.lr_beta > 0.0:\n",
    "        # Jointly train class-specific beta.\n",
    "        # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "        beta = nn.Parameter(nn.init.constant(torch.zeros(len(train_dir.classes)),args.beta))\n",
    "        optimizer_beta = optim.SGD([beta], lr=args.lr_beta,momentum=0.9)    \n",
    "    # optionally resume from a checkpoint\n",
    "#     if args.resume:\n",
    "#         if os.path.isfile(args.resume):\n",
    "#             print('=> loading checkpoint {}'.format(args.resume))\n",
    "#             checkpoint = torch.load(args.resume)\n",
    "#             args.start_epoch = checkpoint['epoch']\n",
    "#         else:\n",
    "#             checkpoint = None\n",
    "#             print('=> no checkpoint found at {}'.format(args.resume))\n",
    "#     print(checkpoint)\n",
    "\n",
    "\n",
    "    start = args.start_epoch\n",
    "    end = start + args.epochs\n",
    "\n",
    "    for epoch in range(start, end):\n",
    "        train(train_loader, model, optimizer,optimizer_beta, epoch)\n",
    "        test(test_loader, model, epoch)\n",
    "        testaccuracy(testaccuracy_loader, model, epoch)\n",
    "#         if test_display_triplet_distance:\n",
    "#             display_triplet_distance_test(model,test_loader,LOG_DIR+\"/test_{}\".format(epoch))\n",
    "#             display_triplet_distance(model,train_loader,LOG_DIR+\"/train_{}\".format(epoch))\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed options:\n",
      "{'data_path': '/scratch/ys3225/deeplearningdataset', 'data_train_path': '/scratch/ys3225/deeplearningdataset/train', 'data_test_path': '/scratch/ys3225/deeplearningdataset/test', 'embed_dim': 128, 'batch_size': 64, 'batch_k': 4, 'gpus': '4', 'epochs': 20, 'optimizer': 'adam', 'lr': 0.0001, 'lr_beta': 0.1, 'margin': 0.2, 'beta': 1.2, 'nu': 0.0, 'factor': 0.5, 'steps': '12,14,16,18', 'wd': 0.0001, 'seed': 123, 'model': 'resnet18_v1', 'save_model_prefix': 'margin_loss_model', 'use_pretrained': False, 'kvstore': 'device', 'log_interval': 20, 'dataroot': '/scratch/ys3225/deeplearningdataset/train', 'testdataroot': '/scratch/ys3225/deeplearningdataset/test', 'lfw_dir': '/scratch/ys3225/lfw', 'lfw_pairs_path': 'lfw_pairs.txt', 'log_dir': '/scratch/ys3225/logdir_triplet_loss', 'resume': '/scratch/ys3225/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', 'start_epoch': 0, 'center_loss_weight': 0.5, 'alpha': 0.5, 'embedding_size': 512, 'test_batch_size': 64, 'n_triplets': 1000000, 'beta1': 0.5, 'lr_decay': 0.0001, 'no_cuda': False, 'gpu_id': '0'}\n",
      "\n",
      "\n",
      "Number of Classes:\n",
      "4988\n",
      "\n",
      "you are using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 0 [0/48625 (0%)]\tLoss: 9.066355: : 0it [00:07, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 0 [0/48625 (0%)]\tLoss: 9.066355: : 1it [00:07,  7.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "*******\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 0 [64/48625 (0%)]\tLoss: 8.998808: : 1it [00:08,  8.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/ys3225/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in less\n",
      "/home/ys3225/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in less\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Epoch: 0 [0/6000 (0%)]: : 0it [00:01, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Epoch: 0 [0/6000 (0%)]: : 1it [00:01,  1.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest Verification set: Accuracy: 0.99166667\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Recognition Epoch: 0 [0/9807 (0%)]\tTest Prec@1 0.00 (0.00): : 0it [00:03, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Recognition Epoch: 0 [0/9807 (0%)]\tTest Prec@1 0.00 (0.00): : 1it [00:03,  3.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/48625 (0%)]\tLoss: 9.029393: : 0it [00:03, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/48625 (0%)]\tLoss: 9.029393: : 1it [00:03,  3.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "*******\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 1 [64/48625 (0%)]\tLoss: 9.052375: : 1it [00:04,  4.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Epoch: 1 [0/6000 (0%)]: : 0it [00:01, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Epoch: 1 [0/6000 (0%)]: : 1it [00:01,  1.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest Verification set: Accuracy: 0.99166667\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Recognition Epoch: 1 [0/9807 (0%)]\tTest Prec@1 0.00 (0.00): : 0it [00:02, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Recognition Epoch: 1 [0/9807 (0%)]\tTest Prec@1 0.00 (0.00): : 1it [00:02,  2.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/48625 (0%)]\tLoss: 9.077905: : 0it [00:04, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/48625 (0%)]\tLoss: 9.077905: : 1it [00:04,  4.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "*******\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 2 [64/48625 (0%)]\tLoss: 9.025120: : 1it [00:05,  5.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Epoch: 2 [0/6000 (0%)]: : 0it [00:01, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Epoch: 2 [0/6000 (0%)]: : 1it [00:01,  1.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AProcess Process-97:\n",
      "Process Process-98:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3f949027440c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mtestaccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestaccuracy_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#     if test_display_triplet_distance:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-47d2501179fc>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(test_loader, model, epoch)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubdist\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\33[91mTest Verification set: Accuracy: {:.8f}\\n\\33[0m'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m#     #logger.log_value('Test Accuracy', np.mean(accuracy))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/facenet_pytorch/Face-recognition/src/eval_metrics.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(distances, labels, nrof_folds)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     val, val_std, far = calculate_val(thresholds, distances,\n\u001b[0;32m---> 13\u001b[0;31m         labels, 1e-3, nrof_folds=nrof_folds)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/facenet_pytorch/Face-recognition/src/eval_metrics.py\u001b[0m in \u001b[0;36mcalculate_val\u001b[0;34m(thresholds, distances, labels, far_target, nrof_folds)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mfar_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrof_thresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthreshold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthreshold_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_val_far\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfar_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mfar_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfar_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'slinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/facenet_pytorch/Face-recognition/src/eval_metrics.py\u001b[0m in \u001b[0;36mcalculate_val_far\u001b[0;34m(threshold, dist, actual_issame)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mfalse_accept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_issame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_issame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mn_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_issame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mn_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_issame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_diff\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mn_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1834\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#def main(): test of main\n",
    "#test_display_triplet_distance= True\n",
    "'''\n",
    "why test_display_triplet_distance= True in center loss.py?????\n",
    "'''\n",
    "test_display_triplet_distance= True\n",
    "# print the experiment configuration\n",
    "print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "print('\\nNumber of Classes:\\n{}\\n'.format(len(train_dir.classes)))\n",
    "\n",
    "# instantiate model and initialize weights\n",
    "#model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "model = FaceModelMargin(embedding_size=args.embedding_size,\n",
    "                  num_classes=len(train_dir.classes),batch_k = args.batch_k, pretrained=False)\n",
    "args.cuda = True\n",
    "if args.cuda:\n",
    "    print(\"you are using gpu\")\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = create_optimizer(model, args.lr)\n",
    "if args.lr_beta > 0.0:\n",
    "        # Jointly train class-specific beta.\n",
    "        # See \"sampling matters in deep embedding learning\" paper for details.\n",
    "        beta = nn.Parameter(nn.init.constant(torch.zeros(len(train_dir.classes)),args.beta))\n",
    "        optimizer_beta = optim.SGD([beta], lr=args.lr_beta,momentum=0.9)\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "#     if args.resume:\n",
    "#         if os.path.isfile(args.resume):\n",
    "#             print('=> loading checkpoint {}'.format(args.resume))\n",
    "#             checkpoint = torch.load(args.resume)\n",
    "#             args.start_epoch = checkpoint['epoch']\n",
    "#         else:\n",
    "#             checkpoint = None\n",
    "#             print('=> no checkpoint found at {}'.format(args.resume))\n",
    "#     print(checkpoint)\n",
    "\n",
    "\n",
    "start = args.start_epoch\n",
    "end = start + args.epochs\n",
    "\n",
    "for epoch in range(start, end):\n",
    "    train(train_loader, model, optimizer,optimizer_beta, epoch)\n",
    "    test(test_loader, model, epoch)\n",
    "    testaccuracy(testaccuracy_loader, model, epoch)\n",
    "#     if test_display_triplet_distance:\n",
    "#         display_triplet_distance_test(model,test_loader,LOG_DIR+\"/test_{}\".format(epoch))\n",
    "#         display_triplet_distance(model,train_loader,LOG_DIR+\"/train_{}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test by cpu\n",
    "epoch = 1\n",
    "#def train(train_loader, model, optimizer, epoch):\n",
    "# switch to train mode\n",
    "model.train()\n",
    "\n",
    "pbar = tqdm(enumerate(train_loader))\n",
    "labels, distances = [], []\n",
    "\n",
    "\n",
    "for batch_idx, (data,label) in pbar:\n",
    "    data = Variable(data)#data = Variable(data.cuda())********************\n",
    "    # compute output\n",
    "    # 需保证anchors, positives, negatives 是variable和cuda\n",
    "    x, a_indices, anchors, positives, negatives = model(data)\n",
    "    # 暂时没有写beta的train， 后面再补\n",
    "    #args.lr_beta = -1\n",
    "    if args.lr_beta > 0.0:\n",
    "        margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, args.beta, label.index_select(0,a_indices.data))\n",
    "    else:\n",
    "        margin_loss = MarginLoss(margin=args.margin, nu=args.nu).forward(anchors, positives, negatives, args.beta, None)\n",
    "    # L should be a scalar\n",
    "    print(margin_loss)\n",
    "    #cumulative_loss += L\n",
    "        #for L in Ls:\n",
    "            #L.backward()\n",
    "# 加上训练Beta的optimizer，后一步再说\n",
    "#             # Update.\n",
    "#             trainer.step(x.shape[0])\n",
    "#             if opt.lr_beta > 0.0:\n",
    "#                 trainer_beta.step(x.shape[0])\n",
    "\n",
    "    predicted_labels = model.forward_classifier(data)#.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    true_labels = Variable(label)#.cuda())\n",
    "\n",
    "    cross_entropy_loss = criterion(predicted_labels,true_labels)\n",
    "\n",
    "    loss = cross_entropy_loss + margin_loss\n",
    "    # compute gradient and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update the optimizer learning rate\n",
    "    adjust_learning_rate(optimizer)\n",
    "\n",
    "    # log loss value\n",
    "    #logger.log_value('triplet_loss', triplet_loss.data[0]).step()\n",
    "    #logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "    #logger.log_value('total_loss', loss.data[0]).step()\n",
    "    if batch_idx % args.log_interval == 0:\n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
