{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed options:\n",
      "{'dataroot': '/scratch/hb1500/Face_Aligned_6400/train_l/', 'testdataroot': '/scratch/hb1500/Face_Aligned_6670/test_l/', 'lfw_dir': '/scratch/hb1500/lfw/lfw', 'lfw_pairs_path': 'lfw_pairs.txt', 'log_dir': '/scratch/hb1500/logdir_center_loss', 'resume': '/scratch/hb1500/logdir_center_loss/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', 'start_epoch': 0, 'epochs': 100, 'center_loss_weight': 0.5, 'alpha': 0.5, 'embedding_size': 512, 'batch_size': 64, 'test_batch_size': 64, 'lr': 0.001, 'beta1': 0.5, 'lr_decay': 0.0001, 'wd': 0.0, 'optimizer': 'adam', 'no_cuda': False, 'gpu_id': '0', 'seed': 0, 'log_interval': 10, 'cuda': True}\n",
      "\n",
      "\n",
      "Number of Classes:\n",
      "6400\n",
      "\n",
      "=> no checkpoint found at /scratch/hb1500/logdir_center_loss/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 0it [00:00, ?it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 1it [00:00,  1.65it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 2it [00:00,  2.56it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 3it [00:01,  2.72it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 4it [00:01,  3.18it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 5it [00:01,  3.15it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 6it [00:01,  3.46it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 7it [00:02,  3.28it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 8it [00:02,  3.49it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 9it [00:02,  3.36it/s]\u001b[A\n",
      "Train Epoch: 0 [0/435200 (0%)]\tLoss: 8.783434\tTrain Prec@1 0.00 (0.00): : 10it [00:02,  3.53it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 10it [00:04,  2.19it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 11it [00:04,  2.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 12it [00:05,  2.10it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 13it [00:07,  1.71it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 14it [00:10,  1.35it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 15it [00:11,  1.35it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 16it [00:13,  1.16it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 17it [00:14,  1.16it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 18it [00:16,  1.06it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 19it [00:17,  1.07it/s]\u001b[A\n",
      "Train Epoch: 0 [640/435200 (0%)]\tLoss: 8.775267\tTrain Prec@1 0.00 (0.00): : 20it [00:21,  1.05s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 20it [00:21,  1.06s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 21it [00:21,  1.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 22it [00:24,  1.12s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 23it [00:24,  1.09s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 24it [00:27,  1.13s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 25it [00:28,  1.14s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 26it [00:29,  1.14s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 27it [00:31,  1.18s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 28it [00:33,  1.20s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 29it [00:34,  1.21s/it]\u001b[A\n",
      "Train Epoch: 0 [1280/435200 (0%)]\tLoss: 8.776377\tTrain Prec@1 0.00 (0.00): : 30it [00:35,  1.20s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 30it [00:37,  1.27s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 31it [00:38,  1.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 32it [00:39,  1.22s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 33it [00:40,  1.23s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 34it [00:41,  1.23s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 35it [00:42,  1.23s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 36it [00:45,  1.25s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 37it [00:45,  1.23s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 38it [00:47,  1.26s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 39it [00:48,  1.24s/it]\u001b[A\n",
      "Train Epoch: 0 [1920/435200 (0%)]\tLoss: 8.772702\tTrain Prec@1 0.00 (0.00): : 40it [00:50,  1.27s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 40it [00:51,  1.29s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 41it [00:51,  1.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 42it [00:54,  1.29s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 43it [00:55,  1.29s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 44it [00:57,  1.31s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 45it [00:59,  1.33s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 46it [01:01,  1.33s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 47it [01:02,  1.33s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 48it [01:04,  1.33s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 49it [01:07,  1.38s/it]\u001b[A\n",
      "Train Epoch: 0 [2560/435200 (1%)]\tLoss: 8.769237\tTrain Prec@1 0.00 (0.00): : 50it [01:08,  1.36s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 50it [01:11,  1.43s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 51it [01:11,  1.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 52it [01:12,  1.40s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 53it [01:16,  1.44s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 54it [01:16,  1.41s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 55it [01:19,  1.44s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 56it [01:20,  1.44s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 57it [01:25,  1.50s/it]\u001b[A\n",
      "Train Epoch: 0 [3200/435200 (1%)]\tLoss: 8.769588\tTrain Prec@1 0.00 (0.00): : 58it [01:25,  1.48s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3ec55819a797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtestRecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestaccuracy_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-3ec55819a797>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtestaccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestaccuracy_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9ad0a977114a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdata_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtarget_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/tqdm-4.19.4-py3.6.egg/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Process Process-27:\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-b1000999c5f9>\", line 48, in __getitem__\n",
      "    img = transform(path)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-1-b1000999c5f9>\", line 44, in transform\n",
      "    img = self.loader(img_path)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hb1500/py3.6.3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/hb1500/py3.6.3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/share/apps/pytorch/0.2.0_3/python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/hb1500/py3.6.3/lib/python3.6/site-packages/PIL/Image.py\", line 2557, in open\n",
      "    prefix = fp.read(16)\n",
      "  File \"<ipython-input-1-b1000999c5f9>\", line 48, in __getitem__\n",
      "    img = transform(path)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-1-b1000999c5f9>\", line 44, in transform\n",
      "    img = self.loader(img_path)\n",
      "  File \"/home/hb1500/py3.6.3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/hb1500/py3.6.3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/home/hb1500/py3.6.3/lib/python3.6/site-packages/PIL/Image.py\", line 2557, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import FaceModel,FaceModelCenter,FaceModelSoftmax\n",
    "from eval_metrics import evaluate\n",
    "from logger import Logger\n",
    "from LFWDataset import LFWDataset\n",
    "from TrainDataset import TrainDataset\n",
    "from PIL import Image\n",
    "from utils import PairwiseDistance,display_triplet_distance,display_triplet_distance_test\n",
    "import collections\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Face Recognition')\n",
    "# Model options\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/hb1500/Face_Aligned_6400/train_l/',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/hb1500/Face_Aligned_6670/test_l/',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/hb1500/lfw/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "\n",
    "parser.add_argument('--log-dir', default='/scratch/hb1500/logdir_center_loss',\n",
    "                    help='folder to output model checkpoints')\n",
    "\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/hb1500/logdir_center_loss/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='E',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "# Training options\n",
    "# parser.add_argument('--embedding-size', type=int, default=256, metavar='ES',\n",
    "#                     help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--center_loss_weight', type=float, default=0.5, help='weight for center loss')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='learning rate of the centers')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='BS',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "parser.add_argument('--wd', default=0.0, type=float,\n",
    "                    metavar='W', help='weight decay (default: 0.0)')\n",
    "parser.add_argument('--optimizer', default='adam', type=str,\n",
    "                    metavar='OPT', help='The optimizer to use (default: Adagrad)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "                    help='random seed (default: 0)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "\n",
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "LOG_DIR_LOG = args.log_dir + '/logger'\n",
    "\n",
    "# create logger\n",
    "logger = Logger(LOG_DIR_LOG)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "l2_dist = PairwiseDistance(2)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize((96,96)),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "#train_dir = ImageFolder(args.dataroot,transform=transform)\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dir,\n",
    "#    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainDataset(dir=args.dataroot,transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_var = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "\n",
    "        center_loss, model.centers = model.get_center_loss(target_var, args.alpha)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        cross_entropy_loss = criterion(prediction.cuda(),target_var.cuda())\n",
    "\n",
    "        loss = args.center_loss_weight*center_loss + cross_entropy_loss\n",
    "        #loss = cross_entropy_loss\n",
    "\n",
    "        # compute gradient and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the optimizer learning rate\n",
    "        adjust_learning_rate(optimizer)\n",
    "\n",
    "        # log loss value\n",
    "        logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "        logger.log_value('center_loss', center_loss.data[0]).step()\n",
    "        logger.log_value('total_loss', loss.data[0]).step()\n",
    "\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t'\n",
    "                'Train Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data[0],float(top1.val[0]), float(top1.avg[0])))\n",
    "                \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t'\n",
    "                'Train Prec@1 {:.2f} ({:.2f})\\n'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data[0],float(top1.val[0]), float(top1.avg[0])))\n",
    "#         if batch_idx ==1:\n",
    "#             break\n",
    "    logger.log_value('Train Prec@1 ',float(top1.avg[0]))\n",
    "\n",
    "    # do checkpointing\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.mkdir(LOG_DIR)\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'centers': model.centers},\n",
    "            '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "        data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "                                Variable(data_p, volatile=True), Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p = model(data_a), model(data_p)\n",
    "        dists = l2_dist.forward(out_a,out_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(label.data.cpu().numpy())\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description('Test LFW Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                100. * batch_idx / len(test_loader)))\n",
    "            #file = open('./log_center_loss/Verification_Accuracy.txt','a') \n",
    "            \n",
    "#         if batch_idx ==1:\n",
    "#             break\n",
    "    #print(distances)\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "    print('\\33[91mTest LFW set: Verification Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    #file.write('\\33[91mTest set: Verification Accuracy: {:.8f}\\n\\33[0m \\n'.format(np.mean(accuracy)))\n",
    "            #file.close()\n",
    "    \n",
    "    \n",
    "    logger.log_value('Test Verification Accuracy', np.mean(accuracy))\n",
    "    \n",
    "    plot_roc(fpr,tpr,figure_name=\"roc_test_epoch_{}.png\".format(epoch))\n",
    "    \n",
    "\n",
    "def testaccuracy(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_value = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Recognition Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(top1.val[0]),float(top1.avg[0]))) \n",
    "            #file = open('./log_center_loss/Recognition_Accuracy.txt','a') \n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Recognition Prec@1 {:.2f} ({:.2f})\\n'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(top1.val[0]),float(top1.avg[0])))\n",
    "            #file.close()\n",
    "#         if batch_idx == 1:\n",
    "#             break\n",
    "\n",
    "    logger.log_value('Test batch Recognition Accuracy', float(top1.val[0]))            \n",
    "    logger.log_value('Test total Recognition Accuracy', float(top1.avg[0]))\n",
    "        \n",
    "\n",
    "def testRecall(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data = Variable(data.cuda())\n",
    "        # compute output\n",
    "        out_data = model(data)\n",
    "        distance_matrix = get_distance(out_data.cpu().data.numpy())\n",
    "        labels = label.cpu().numpy()\n",
    "        \n",
    "        names = []\n",
    "        accs = []\n",
    "        \n",
    "        for k in [1, 2, 4, 8, 16]:\n",
    "            names.append('Recall@%d' % k)\n",
    "            correct, cnt = 0.0, 0.0\n",
    "            for i in range(len(data)):\n",
    "                distance_matrix[i, i] = 1e10\n",
    "                nns = np.argpartition(distance_matrix[i], k)[:k]\n",
    "                if any(labels[i] == labels[nn] for nn in nns):\n",
    "                    correct += 1\n",
    "                cnt += 1\n",
    "            accs.append(correct/cnt)\n",
    "        \n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Recall@1  {:.2f} \\n'.format(\n",
    "                    epoch, batch_idx * len(data), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(accs[0])))\n",
    "#             file = open('./log_triplet_loss/Recognition_Recall.txt','a') \n",
    "#             file.write('Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "#                 'Test Recognition Prec@1 {:.2f} \\n'.format(\n",
    "#                     epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "#                     100. * batch_idx / len(test_loader),\n",
    "#                     float(accs[0])))\n",
    "#             file.close()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:                  \n",
    "            logger.log_value('Test Recall (l)', accs[0])\n",
    "            logger.log_value('Test Recall (2)', accs[1])\n",
    "            logger.log_value('Test Recall (4)', accs[2])\n",
    "            logger.log_value('Test Recall (8)', accs[3])\n",
    "            logger.log_value('Test Recall (16)', accs[4])\n",
    "#         if batch_idx ==1:\n",
    "#             break\n",
    "def get_distance(x):\n",
    "    \"\"\"Helper function for margin-based loss. Return a distance matrix given a matrix.\"\"\"\n",
    "    n = x.shape[0]\n",
    "    square = np.sum(x ** 2,1,keepdims =True)\n",
    "    distance_square = square + square.T - (2.0 * np.dot(x, x.T))\n",
    "    # Adding identity to make sqrt work.\n",
    "    return np.sqrt(distance_square + np.identity(n))\n",
    "           \n",
    "def plot_roc(fpr,tpr,figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(os.path.join(LOG_DIR,figure_name), dpi=fig.dpi)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0\n",
    "        group['step'] += 1\n",
    "\n",
    "        group['lr'] = args.lr / (1 + group['step'] * args.lr_decay)\n",
    "\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=args.wd, betas=(args.beta1, 0.999))\n",
    "    elif args.optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(),\n",
    "                                  lr=new_lr,\n",
    "                                  lr_decay=args.lr_decay,\n",
    "                                  weight_decay=args.wd)\n",
    "    return optimizer\n",
    "\n",
    "def main():\n",
    "    # print the experiment configuration\n",
    "    print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "    print('\\nNumber of Classes:\\n{}\\n'.format(str(6400)))\n",
    "    num_classes = 6400\n",
    "    # # instantiate model and initialize weights\n",
    "\n",
    "\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print('=> loading checkpoint {}'.format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            checkpoint = None\n",
    "            print('=> no checkpoint found at {}'.format(args.resume))\n",
    "    #print(checkpoint)\n",
    "    #model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "    model = FaceModelCenter(embedding_size=args.embedding_size,num_classes=num_classes,checkpoint=None)\n",
    "    if args.cuda:\n",
    "        #print(\"you are using gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = create_optimizer(model, args.lr)\n",
    "\n",
    "    start = args.start_epoch\n",
    "    end = start + args.epochs\n",
    "    for epoch in range(start, end):\n",
    "        train(train_loader, model, optimizer, epoch)\n",
    "        test(test_loader, model, epoch)\n",
    "        testaccuracy(testaccuracy_loader, model, epoch)\n",
    "        testRecall(testaccuracy_loader, model, epoch)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
