{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f43ea8238cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import FaceModel,FaceModelCenter,FaceModelSoftmax\n",
    "from eval_metrics import evaluate\n",
    "#from logger import Logger\n",
    "from LFWDataset import LFWDataset\n",
    "from PIL import Image\n",
    "from utils import PairwiseDistance,display_triplet_distance,display_triplet_distance_test\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Face Recognition')\n",
    "# Model options\n",
    "parser.add_argument('--dataroot', type=str, default='/scratch/hb1500/lfw/lfw',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--testdataroot', type=str, default='/scratch/hb1500/lfw/lfw',#default='/media/lior/LinuxHDD/datasets/vgg_face_dataset/aligned'\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-dir', type=str, default='/scratch/hb1500/lfw/lfw',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--lfw-pairs-path', type=str, default='lfw_pairs.txt',\n",
    "                    help='path to pairs file')\n",
    "\n",
    "parser.add_argument('--log-dir', default='/scratch/hb1500/logdir',\n",
    "                    help='folder to output model checkpoints')\n",
    "\n",
    "parser.add_argument('--resume',\n",
    "                    default='/scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth',\n",
    "                    type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--epochs', type=int, default=1, metavar='E',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "# Training options\n",
    "# parser.add_argument('--embedding-size', type=int, default=256, metavar='ES',\n",
    "#                     help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--center_loss_weight', type=float, default=0.5, help='weight for center loss')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='learning rate of the centers')\n",
    "parser.add_argument('--embedding-size', type=int, default=512, metavar='ES',\n",
    "                    help='Dimensionality of the embedding')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='BS',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=64, metavar='BST',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "\n",
    "parser.add_argument('--lr-decay', default=1e-4, type=float, metavar='LRD',\n",
    "                    help='learning rate decay ratio (default: 1e-4')\n",
    "parser.add_argument('--wd', default=0.0, type=float,\n",
    "                    metavar='W', help='weight decay (default: 0.0)')\n",
    "parser.add_argument('--optimizer', default='adam', type=str,\n",
    "                    metavar='OPT', help='The optimizer to use (default: Adagrad)')\n",
    "# Device options\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "                    help='random seed (default: 0)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='LI',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:01<00:00, 3791.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "if args.cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "LOG_DIR = args.log_dir + '/run-optim_{}-lr{}-wd{}-embeddings{}-center_loss{}-MSCeleb'.format(args.optimizer, args.lr, args.wd,args.embedding_size,args.center_loss_weight)\n",
    "\n",
    "\n",
    "# create logger\n",
    "#logger = Logger(LOG_DIR)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "l2_dist = PairwiseDistance(2)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                         transforms.Resize(96),\n",
    "                         transforms.RandomHorizontalFlip(),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n",
    "\n",
    "train_dir = ImageFolder(args.dataroot,transform=transform)\n",
    "testacc_dir = ImageFolder(args.testdataroot,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "testaccuracy_loader = torch.utils.data.DataLoader(testacc_dir,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=args.lfw_dir,pairs_path=args.lfw_pairs_path,\n",
    "                     transform=transform),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed options:\n",
      "{'dataroot': '/scratch/hb1500/lfw/lfw', 'testdataroot': '/scratch/hb1500/lfw/lfw', 'lfw_dir': '/scratch/hb1500/lfw/lfw', 'lfw_pairs_path': 'lfw_pairs.txt', 'log_dir': '/scratch/hb1500/logdir', 'resume': '/scratch/hb1500/resume/run-optim_adam-lr0.001-wd0.0-embeddings512-center0.5-MSCeleb/checkpoint_11.pth', 'start_epoch': 0, 'epochs': 1, 'center_loss_weight': 0.5, 'alpha': 0.5, 'embedding_size': 512, 'batch_size': 64, 'test_batch_size': 64, 'lr': 0.001, 'beta1': 0.5, 'lr_decay': 0.0001, 'wd': 0.0, 'optimizer': 'adam', 'no_cuda': False, 'gpu_id': '0', 'seed': 0, 'log_interval': 10}\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7807171b72cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-7807171b72cf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# print the experiment configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nparsed options:\\n{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nNumber of Classes:\\n{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# instantiate model and initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_var = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "\n",
    "        center_loss, model.centers = model.get_center_loss(target_var, args.alpha)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        cross_entropy_loss = criterion(prediction.cuda(),target_var.cuda())\n",
    "\n",
    "        loss = args.center_loss_weight*center_loss + cross_entropy_loss\n",
    "        #loss = cross_entropy_loss\n",
    "\n",
    "        # compute gradient and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the optimizer learning rate\n",
    "        adjust_learning_rate(optimizer)\n",
    "\n",
    "        # log loss value\n",
    "        #logger.log_value('cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "        # logger.log_value('total_loss', loss.data[0]).step()\n",
    "\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t'\n",
    "                'Train Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data[0],float(top1.val[0]), float(top1.avg[0])))\n",
    "\n",
    "\n",
    "\n",
    "    #logger.log_value('Train Prec@1 ',float(top1.avg[0]))\n",
    "\n",
    "    # do checkpointing\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.mkdir(LOG_DIR)\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'centers': model.centers},\n",
    "            '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "        data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "                                Variable(data_p, volatile=True), Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p = model(data_a), model(data_p)\n",
    "        dists = l2_dist.forward(out_a,out_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy())\n",
    "        labels.append(label.data.cpu().numpy())\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description('Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                100. * batch_idx / len(test_loader)))\n",
    "    #print(distances)\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances,labels)\n",
    "    print('\\33[91mTest set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    #logger.log_value('Test Accuracy', np.mean(accuracy))\n",
    "\n",
    "    plot_roc(fpr,tpr,figure_name=\"roc_test_epoch_{}.png\".format(epoch))\n",
    "\n",
    "def testaccuracy(test_loader,model,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    top1 = AverageMeter()\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data_v = Variable(data.cuda())\n",
    "        target_value = Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        prediction = model.forward_classifier(data_v)\n",
    "        prec = accuracy(prediction.data, label.cuda(), topk=(1,))\n",
    "        top1.update(prec[0], data_v.size(0))\n",
    "        #correct += accuracy(prediction.data, label.cuda(), topk=(1,))[0]*data_v.size(0)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Test Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                'Test Prec@1 {:.2f} ({:.2f})'.format(\n",
    "                    epoch, batch_idx * len(data_v), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    float(top1.val[0]),float(top1.avg[0]))) \n",
    "    \n",
    "def plot_roc(fpr,tpr,figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(os.path.join(LOG_DIR,figure_name), dpi=fig.dpi)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0\n",
    "        group['step'] += 1\n",
    "\n",
    "        group['lr'] = args.lr / (1 + group['step'] * args.lr_decay)\n",
    "\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=args.wd, betas=(args.beta1, 0.999))\n",
    "    elif args.optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(),\n",
    "                                  lr=new_lr,\n",
    "                                  lr_decay=args.lr_decay,\n",
    "                                  weight_decay=args.wd)\n",
    "    return optimizer\n",
    "\n",
    "def main():\n",
    "    #test_display_triplet_distance= True\n",
    "    '''\n",
    "    why test_display_triplet_distance= True in center loss.py?????\n",
    "    '''\n",
    "    test_display_triplet_distance= False\n",
    "    # print the experiment configuration\n",
    "    print('\\nparsed options:\\n{}\\n'.format(vars(args)))\n",
    "    print('\\nNumber of Classes:\\n{}\\n'.format(len(train_dir.classes)))\n",
    "\n",
    "    # instantiate model and initialize weights\n",
    "\n",
    "\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print('=> loading checkpoint {}'.format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            checkpoint = None\n",
    "            print('=> no checkpoint found at {}'.format(args.resume))\n",
    "    print(checkpoint)\n",
    "    #model = FaceModelSoftmax(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "    model = FaceModelCenter(embedding_size=args.embedding_size,num_classes=len(train_dir.classes),checkpoint=checkpoint)\n",
    "    if args.cuda:\n",
    "        print(\"you are using gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = create_optimizer(model, args.lr)\n",
    "\n",
    "    start = args.start_epoch\n",
    "    end = start + args.epochs\n",
    "\n",
    "    for epoch in range(start, end):\n",
    "        train(train_loader, model, optimizer, epoch)\n",
    "        test(test_loader, model, epoch)\n",
    "        testaccuracy(testaccuracy_loader, model, epoch)\n",
    "        if test_display_triplet_distance:\n",
    "            display_triplet_distance_test(model,test_loader,LOG_DIR+\"/test_{}\".format(epoch))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
